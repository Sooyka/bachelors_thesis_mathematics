\documentclass[a4paper]{article}
\usepackage{polski}
\usepackage[polish,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[pdftex,
            pdfauthor={Bartosz Sójka},
            pdftitle={Praca licencjacka brudnopis},
            pdfsubject={Explanation of connection between Hopf algebras and Makov chains}]{hyperref}

%\setlength\parindent{0pt}

\newtheorem{observation}{Observation}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newcommand{\gdd}[1]{#1^{\mathrm{gd}*}}
\newcommand{\todo}[1]{\hfill \break \textbf{\Huge TO DO: #1 \hfill \break}\normalsize}
\newcommand{\smalltodo}[1]{\textbf{\ To do}}
\newcommand{\onedotsn}[1]{{#1}_1, \dots, {#1}_n}
\newcommand{\ndotsm}[3]{{#1}_{#2}, \dots, {#1}_{#3}}
\newcommand{\SimeqSym}{{\simeq_\mathrm{sym}}}
\title{Explanation of connection between Hopf algebras and Markov chains}
\author{Bartosz Sójka}
\date{May 2018}

\begin{document}
\begin{align}
[z]_{/\simeq} = \left[\sum^n_{i = 1}\alpha_i(v_i,w_i)\right]_{/\simeq} &=
\displaystyle\sum^n_{i = 1}\alpha_i[(v_i,w_i)]_{/\simeq} = \\
\displaystyle\sum^n_{i = 1}\alpha_i\left[\left(\sum^{l_1}_{j=1}\gamma_{i,j}b_{i,j},
\sum^{l_2}_{k=1}\gamma_{i,k}c_{i,k}\right)\right]_{/\simeq} &=
\displaystyle\sum^n_{i = 1}\alpha_i\left[\sum^{l_1}_{j=1}\gamma_{i,j}\left(b_{i,j},
\sum^{l_2}_{k=1}\gamma_{i,k}c_{i,k}\right)\right]_{/\simeq} = \\
\displaystyle\sum^n_{i = 1}\alpha_i\left[\sum^{l_1}_{j=1}\gamma_{i,j}\left(\sum^{l_2}_{k=1}\gamma_{i,k}
\left(b_{i,j},c_{i,k}\right)\right)\right]_{/\simeq} &=
\displaystyle\sum^n_{i = 1}\alpha_i\left[\sum^{l_1}_{\substack{1 \leq j \leq l_1 \\ 1 \leq k \leq l_2}}
\gamma_{i,j}\gamma_{i,k}(b_{i,j}, c_{i,k})\right]_{/\simeq}
\end{align}

\begin{equation}
\sum^n_{\substack{1 \leq i \leq n \\1 \leq j \leq l_1 \\ 1 \leq k \leq l_2}}
\alpha_i\gamma_{i,j}\gamma_{i,k}[(b_{i,j}, c_{i,k})]_{/\simeq})_{/\simeq}
\end{equation}

Then take $\displaystyle\sum^n_{i=1}(v_i,w_i)$. There are no $v, w$
such that $ [(v,w)]_{/\simeq} = [(v_1,w_1) +\dots + (v_n,w_n)]_{/\simeq}$.
Thus for the element $[(v_1,w_1) + \dots + (v_n,w_n)]_{/\simeq}$ of $V\otimes W$ there are no $v, w$
such that $v \otimes w = [(v_1,w_1) + \dots + (v_n,w_n)]_{/\simeq}$. However, since
$[(v_1,w_1) + \dots + (v_n,w_n)]_{/\simeq} = [(v_1,w_1)]_{/\simeq} + \dots + [(v_n,w_n)]_{/\simeq}$
it can be writen as $v_1 \otimes w_1 + \dots + v_n \otimes w_n$. \\
Now we will make some futher observations on how $V \otimes W$ looks like.


 is a vector space created from
 of elemets of form $\{v \otimes w : v \in V, w \in W\}$
such that for all $v,v_1,v_2 \in V$, $w, w_1, w_2 \in W$, $k \in K$ there hold
\begin{gather*}
v \otimes w_1 + v \otimes w_2 = v \otimes (w_1 + w_2) \\
v_1 \otimes w + v_2 \otimes w = (v_1 + v_2) \otimes w \\
k (v \otimes w) = (kv) \otimes w = v \otimes (kw)
\end{gather*}
with basises $\{v_i\}_{i \in I}$, $\{w_j\}_{j \in J}$
with basis and operations defined


As algebra we will understand a vector space $\mathcal{H}$ with additional linear operation
\text{$m:\mathcal{H} \otimes \mathcal{H} \rightarrow \mathcal{H}$} called multiplication.
Natural example are polinomials. Trivial example is multimlication within a field.
Now we will introduce an important example of algebra of noncommutative variables. Later it will rise
to Hopf algebra we will be using for describing inverse riffle shuffle.



In fact this is the purpose of map $u$ to insert the copy of the field $K$ into a $K$-algebra.

("$1a$" means $a$ from $\mathcal{H}$ multiplicated by $1$ from the field $K$.)


means exactly what was in the definition of $u$.
Which mean that "counit does the same thing to comultiplication as unit to the multiplication" ;)

Since
\begin{align*}
(\Delta \otimes I)\Delta(c) &= (\Delta \otimes I)\left(\sum c_1 \otimes c_2 \right) =
\sum\Delta(c_1) \otimes c_2, \\
(I \otimes \Delta)\Delta(c) &= (I \otimes \Delta)\left(\sum c_1 \otimes c_2 \right) =
\sum c_1 \otimes \Delta(c_2).
\end{align*}

%\gebin{equation*}

$(\mathcal{H}, m, u)$

$(\mathcal{H}, \Delta, \varepsilon)$


    The canonical isomorphism between $K \otimes K$ and $K$ for all
$k_1, k_2 \in K$ taking a form $k_1 \otimes k_2 \xmapsto{\cong} k_1k_2$ will be writen as $\varphi_K$.

 and because of that we will omit it and identify $K \otimes K$ with $K$
(and, because of associativity of a field multiplication we will identify any power $K^{\otimes n}$ with $K$).
\\[8pt]

Note, that later, when there will be no risk of confusion,
we will be still using "$\cdot$" as multiplication on algebra setted by "$m$" from that algebra.

\textbf{Explanation. } In fact for a given vector space $\mathcal{H}$ with both an algebra structure
$(\mathcal{H}, m, u)$ and a coalgebra structure $(\mathcal{H}, \Delta, \varepsilon)$, $m$ and $u$
are morfisms of coalgebras iff $\Delta$ and $\varepsilon$ are morfisms of algebras and these are equivalent
to conjuction of contditions that

preceisly.


where $I^m$ means $\overbrace{I \otimes \dots \otimes I}^{m \mathrm{\ times}}$. \\

\begin{align*}
[z]_{/\simeq} = \left[\sum^n_{i = 1}\alpha_i(v_i,w_i)\right]_{/\simeq} &=
\sum^n_{i = 1}\alpha_i[(v_i,w_i)]_{/\simeq} = \\
\sum^n_{i = 1}\alpha_i\left[\left(\sum^{l_1}_{j=1}\gamma_{i,j}b_{i,j},
\sum^{l_2}_{k=1}\gamma_{i,k}c_{i,k}\right)\right]_{/\simeq} &=
\sum^n_{i = 1}\alpha_i\left[\sum^{l_1}_{j=1}\gamma_{i,j}\left(b_{i,j},
\sum^{l_2}_{k=1}\gamma_{i,k}c_{i,k}\right)\right]_{/\simeq} = \\
\sum^n_{i = 1}\alpha_i\left[\sum^{l_1}_{j=1}\gamma_{i,j}\left(\sum^{l_2}_{k=1}\gamma_{i,k}
\left(b_{i,j},c_{i,k}\right)\right)\right]_{/\simeq} &=
\sum^n_{i = 1}\alpha_i\left[\sum_{\substack{1 \leq j \leq l_1 \\ 1 \leq k \leq l_2}}
\gamma_{i,j}\gamma_{i,k}(b_{i,j}, c_{i,k})\right]_{/\simeq} = \\
\sum^n_{i = 1}\alpha_i\left(\sum_{\substack{1 \leq j \leq l_1 \\ 1 \leq k \leq l_2}}
\gamma_{i,j}\gamma_{i,k}[(b_{i,j}, c_{i,k})]_{/\simeq}\right) &=
\sum_{\substack{1 \leq i \leq n \\1 \leq j \leq l_1 \\ 1 \leq k \leq l_2}}
\alpha_i\gamma_{i,j}\gamma_{i,k}[(b_{i,j}, c_{i,k})]_{/\simeq}
\end{align*}

\begin{align*}
[z]_{/\simeq} = \left[\sum^n_{i = 1}\alpha_i(v_i,w_i)\right]_{/\simeq} &=
\sum^n_{i = 1}\alpha_i[(v_i,w_i)]_{/\simeq} \\
&= \sum^n_{i = 1}\alpha_i\left[\left(\sum^{l_1}_{j=1}\gamma_{i,j}b_{i,j},
\sum^{l_2}_{k=1}\gamma_{i,k}c_{i,k}\right)\right]_{/\simeq} \\
&= \sum^n_{i = 1}\alpha_i\left[\sum^{l_1}_{j=1}\gamma_{i,j}\left(b_{i,j},
\sum^{l_2}_{k=1}\gamma_{i,k}c_{i,k}\right)\right]_{/\simeq} \\
&= \sum^n_{i = 1}\alpha_i\left[\sum^{l_1}_{j=1}\gamma_{i,j}\left(\sum^{l_2}_{k=1}\gamma_{i,k}
\left(b_{i,j},c_{i,k}\right)\right)\right]_{/\simeq} \\
&= \sum^n_{i = 1}\alpha_i\left[\sum_{\substack{1 \leq j \leq l_1 \\ 1 \leq k \leq l_2}}
\gamma_{i,j}\gamma_{i,k}(b_{i,j}, c_{i,k})\right]_{/\simeq} \\
&= \sum^n_{i = 1}\alpha_i\left(\sum_{\substack{1 \leq j \leq l_1 \\ 1 \leq k \leq l_2}}
\gamma_{i,j}\gamma_{i,k}[(b_{i,j}, c_{i,k})]_{/\simeq}\right) \\
&= \sum_{\substack{1 \leq i \leq n \\1 \leq j \leq l_1 \\ 1 \leq k \leq l_2}}
\alpha_i\gamma_{i,j}\gamma_{i,k}[(b_{i,j}, c_{i,k})]_{/\simeq}
\end{align*}

\begin{align*}
(u\varepsilon *f)(c) &= \sum u\varepsilon(c_1) \cdot f(c_2) \\
&= \sum \varepsilon(c_1)1_A \cdot f(c_2) \\
&= \sum 1_A \cdot \varepsilon(c_1)f(c_2) \\
&= 1_A \cdot \sum \varepsilon(c_1)f(c_2) \\
&= 1_A \cdot f(c) = f(c)
\end{align*}

\begin{align*}
(f * u\varepsilon)(c) &= \sum f(c_1) \cdot u\varepsilon(c_2) \\
&= \sum f(c_1) \cdot \varepsilon(c_2)1_A \\
&= \sum f(c_1)\varepsilon(c_2) \cdot 1_A \\
&= \left(\sum f(c_1)\varepsilon(c_2)\right)\cdot 1_A \\
&= f(c) \cdot 1_A = f(c)
\end{align*}


Hence the name. \\

"with respect to coordinates" \\

in next steps \\

That means that for bialgebra $\mathcal{H}$, a linear map $S \in Hom(\mathcal{H}^C, \mathcal{H}^A)$
is an antipode iff it satisfies

\begin{align*}
\Delta(x_{i_0}\dots x_{i_k}) &= \Delta m^{[k]}(x_{i_0} \otimes \dots \otimes x_{i_k}) \\
&= (m^{[k]} \otimes m^{[k]}) \left(\sum (x_{i_0})_1 \otimes \dots \otimes (x_{i_k})_1 \otimes
(x_{i_0})_2 \otimes \dots \otimes (x_{i_k})_2\right) \\
&= \sum (x_{i_0})_1 \dots (x_{i_k})_1 \otimes
(x_{i_0})_2 \dots (x_{i_k})_2 \\
&= \sum_{S \subseteq \{i_0, \dots i_k\}} \prod_{j \in S} x_j \otimes \prod_{j \notin S} x_j.
\end{align*}

Let $\nu = (\nu_1, \dots \nu_N) \in \mathbb{N}^N$ be our deck of $N$ cards (we just pick $N$ cards from
$\mathcal{X}$ in gene

Let $\nu = (\nu_1, \dots \nu_N) \in \mathbb{N}^N$ be our deck of $N$ cards (we just pick $N$ cards from
$\mathcal{X}$ in general case they can repeat

 comes with certain probability.

 we dont know how exactly these stacks looks
like but

if we have two stacks (we will refer to them as ($L$)eft and ($R$)ight).

and we have with probability $p$ $s_1$ on $L$ and $s$ on $R$ and with probability $1-p$ $s_2$ on $L$ and
$s$ on $R$ is exactly the same situation as having

Note that for all $k_1, \dots, k_n \in K$, such that at least one of them is non-zero,
an expression $\displaystyle \sum^{n}_{i = 0}k_i ($

anallogly

Let denote that pulling apart as a $\Delta : \mathcal{H} \to \mathcal{H} \otimes \mathcal{H}$, which for all
$x_{i_0}, \dots, x_{i_n}$ gives
\begin{equation*}
\Delta(x_{i_0}\cdots x_{i_n}) = \sum_{\substack{S \subseteq
\{ i_0, \dots i_n \} \\ S = \{i_{j_1}, \dots, i_{j_l}\} \\ S^c = \{i_{k_1}, \cdots, i_{k_{n-l}} \}}}
x_{i_{j_1}}\cdots x_{i_{j_l}} \otimes x_{i_{k_1}} \cdots x_{i_{k_{n-l}}}.
\end{equation*}
For putting two piles back together by placing left on the top let us write a linear map
$m : \mathcal{H} \otimes \mathcal{H} \to \mathcal{H}$ that is concatenation, which means, that
for all $x_{i_1}, \dots, x_{i_k}, x_{j_1}, \dots, x_{j_l} \in \mathcal{X}$
\begin{equation*}
m(x_{i_1}\dots x_{i_k} \otimes x_{j_1}\dots x_{j_l}) = x_{i_1}\ldots x_{i_k}x_{j_1}\ldots x_{j_l}.
\end{equation*}

Let denote that pulling apart as a $\Delta : \mathcal{H} \to \mathcal{H} \otimes \mathcal{H}$, then for all
$s \in \mathcal{X}^*$ it will give
\begin{equation*}
\Delta(s) = \sum_{\substack{s_1 \prec s \\ s_2 = s/s_1}}
s_1 \otimes s_2.
\end{equation*}
For putting two piles back together by placing left on the top let us write a linear map
$m : \mathcal{H} \otimes \mathcal{H} \to \mathcal{H}$ that is concatenation, which means, that
for all $s_1, s_2 \in \mathcal{X}^*$
\begin{equation*}
m(s_1 \otimes s_2) = s_1s_2.
\end{equation*}
 all $s = x_{i_1}\dots x_{i_n} \in \mathcal{X}^*$
that meet the condition:

\begin{equation*}
\mathbb{P}{F_{n+1} = s_2 \mid F_n = s_1 } = \mathbb{I_{n+1} = s_1 \mid I_n = s_2}.
\end{equation*}

, we percive it as a certain
amount of stacks

$\backslash\backslash$
Indeed it turned out that a vector space of

At this point we can start to think that it is convinient to interprete a space of possible arragements
of two stacks of cards as a $\mathcal{H} \otimes \mathcal{H}$. \\[4pt]

So we need a vector space with basis made of pairs of basis vectors from $\mathcal{H}$ with actions on them
that are linear to both coordinates. A tensor product $\mathcal{H} \otimes \mathcal{H}$
is the less-degenerated vector space with that properties.\\[4pt]

We will now show, that what we just had defined indeed give the same results as standard model of inverse
riffle shuffling.
In inverse riffle shuffling

Which also equvalent to that matrixes of $(F_i)_{i \geq 0}$ and $(I_i)_{i \geq 0}$ are transpositions of
each other.

a $\backslash$ big! $\backslash$ huge! $\backslash$ giant!

We will be working of an examples of riffle shuffle and inverse riffle shuffle cards shuffling as
our Markov chains. \\
How we will put them in the algebraic way? \\
First we will do this with inverse riffle shuffle, the forward riffle shuffle will then apear in a natural
way. \\


Earlier it was said that comultiplication can be sometimes viewed as a sum
of possible divisions into smaller objects. It happends naturally when we are working with graded bialgebras.
Like in example of polinomials, where natural grading is by degree. \\

\subsubsection{Graded, connected Hopf algebra of non-commuting variables}
This is a main example of our interest. \\
Let K be a field with characteristic 0.
Let $\mathcal{X} = \{x_1, \dots, x_N\}$ be a finite set. For every $n \in \mathbb{N}$ let
$H_n$ be a vector space having as a basis all words of length $n$ made of elements
of $\mathcal{X}$. Let $\mathcal{H} \coloneqq \displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i$.
Let $m : \mathcal{H} \otimes \mathcal{H} \to \mathcal{H}$ be concatenation of words,
that is, for all $k, l \in \mathbb{N}$
for all $x_{i_0}, \dots, x_{i_k}, x_{j_0}, \dots, x_{j_l} \in \mathcal{X}$
\begin{equation*}
m(x_{i_0}\dots x_{i_k} \otimes x_{j_0}\dots x_{j_l}) \coloneqq
x_{i_0}\ldots x_{i_k}x_{j_0}\ldots x_{j_l}.
\end{equation*}
Let $\Delta : \mathcal{H} \to \mathcal{H} \otimes \mathcal{H}$ be defined for all elements from
$\mathcal{X}$ as
\begin{equation*}
\Delta(x_i) = x_i \otimes 1_\mathcal{H} + 1_\mathcal{H} \otimes x_i.
\end{equation*}
and extends lineary and multiplically \\
\textbf{Lemma. } Then $\mathcal{H}$ is the a graded, connected Hopf algebra that is cocomutative.
\begin{proof}
Associativity of $m$ and coassociativity of $\Delta$ are obvious. Actions fit together,
because we define them so. Algebra is graded and connnected because it is.
\end{proof}
We can see $\Delta(x_{i_0}\dots x_{i_k})$ how looks like:
\begin{align*}
\Delta(x_{i_0}\dots x_{i_k}) &= \Delta m^{[k]}(x_{i_0} \otimes \dots \otimes x_{i_k}) \\
&= (m^{[k]} \otimes m^{[k]}) \left(\sum (x_{i_0})_1 \otimes \dots \otimes (x_{i_k})_1 \otimes
(x_{i_0})_2 \otimes \dots \otimes (x_{i_k})_2\right) \\
&= \sum (x_{i_0})_1 \dots (x_{i_k})_1 \otimes
(x_{i_0})_2 \dots (x_{i_k})_2 \\
&= \sum_{S \subseteq \{ i_0, \dots i_k \} } \prod_{j \in S} x_j \otimes \prod_{j \notin S} x_j.
\end{align*}
where $S$ is a multiset, because some of the $i_0, \dot i_k$ can be the same.
Form of that coproduct is like that because when for all $x_i \in \mathcal{X}$ we have
$\Delta(x_i) = x_i \otimes 1_\mathcal{H} + 1_\mathcal{H} \otimes x_i$ then \\[8pt]

a ! \large! \Large! \huge! \Huge! \normalsize

\section{Summarize}
$\displaystyle\sum\ \bigoplus\ +\ \oplus$

\section{Productise}
$\displaystyle\prod\ \bigotimes\ \bigodot\ \times\ *$
\hfill \break
To jest źle: \\
Please note, that at first it can be not clear that $h^*m \in \mathcal{H}^* \otimes \mathcal{H}^*$.
For sure $h^* \in (\mathcal{H} \otimes \mathcal{H})^*$ but in general case we have
$ \mathcal{H}^* \otimes \mathcal{H}^* \subseteq (\mathcal{H} \otimes \mathcal{H})^*$ (with inclusion given by
canonical injection) but not necessarily
$ \mathcal{H}^* \otimes \mathcal{H}^* = (\mathcal{H} \otimes \mathcal{H})^*$.
Although we are in graded bialgebra $\mathcal{H}$ and because of that we know that
$h^*m \in \displaystyle\left(\bigoplus^\infty_{n=0}\mathcal{H}_n \otimes \mathcal{H}_n\right)^* =
\displaystyle\bigoplus^\infty_{n = 0} (\mathcal{H}_n \otimes \mathcal{H}_n)^*$. Futher, for all
$n \in \mathbb{N}$ since $\mathcal{H}_n$ is finite-dimentional we have that
$(\mathcal{H}_n \otimes \mathcal{H}_n)^* = \mathcal{H}_n^* \otimes \mathcal{H}_n^*$.
Hence
\begin{equation*}
h^*m \in \displaystyle\bigoplus^\infty_{n = 0} \mathcal{H}_n^* \otimes \mathcal{H}^*_n =
\mathcal{H}^* \otimes \mathcal{H}^*.
\end{equation*}

Now we will describe how a dual algebra to $\mathcal{H}$ looks like. \\
Let $\mathcal{H}^*$ be the dual vector space to $\mathcal{H}$. (A space of linear functions from
$\mathcal{H}$ to $K$). We define multiplication
$\Delta^* : \mathcal{H}^* \otimes \mathcal{H}^* \to \mathcal{H}^*$ and comultiplication
$m^* : \mathcal{H}^* \to \mathcal{H}^* \otimes \mathcal{H}^*$ as
(for all $h_1^*, h_2^*, h^* \in \mathcal{H}^*$):
\begin{align*}
\Delta^*(h_1^* \otimes h_2^*) = (h_1^* \otimes h_2^*)\Delta, \\
m^*(h^*) = h^*m.
\end{align*}

Structure detail described in this paragraph will be important in chapter about eigenbasises.
[GR89] shows that symmetrized sums of certain primitive elements form basis of a free associative algebra.
It will turn out that this will be left eigenbasis of $m\Delta$. Now we will introduce methods for
construction of that basis. Explanation why this is an eigenbasis will came in Chapter 4. This whole
paragraph is exactly the same as in ~\cite{Diaconis2014} I put it here because it is quite short and \\

\begin{align*}
\Psi^{[i]}(h) = m^{[i]}\Delta^{[i]}(h) &= \\
m^{[i]}(\sum h_1 \otimes \dots \otimes h_i) &= \\
m^{[i]}(\underbrace{\overbrace{h \otimes 1_\mathcal{H} \otimes \dots \otimes 1_\mathcal{H}}^{i\
\mathrm{factors}} +
\overbrace{1_\mathcal{H} \otimes h \otimes \dots \otimes 1_\mathcal{H}}^{i\ \mathrm{factors}} +
\dots +
\overbrace{1_\mathcal{H} \otimes 1_\mathcal{H} \otimes \dots \otimes h}^{i\ \mathrm{factors}}}_{i\
\mathrm{summands}} ) &= \\
\underbrace{m^{[i]}(\overbrace{h \otimes 1_\mathcal{H} \otimes \dots \otimes 1_\mathcal{H}}^{i\
\mathrm{factors}}) +
m^{[i]}(\overbrace{1_\mathcal{H} \otimes h \otimes \dots \otimes 1_\mathcal{H}}^{i\ \mathrm{factors}}) +
\dots +
m^{[i]}(\overbrace{1_\mathcal{H} \otimes 1_\mathcal{H} \otimes \dots \otimes h}^{i\ \mathrm{factors}})}_{i\
\mathrm{summands}} &=
\end{align*}

For simplifying the notation we will write a symbol for algebra multiplication also for componentwise
multiplication, so for all ${^1h}, \dots, {^nh} \in \mathcal{H}$:
\begin{equation}
\Delta ^{[m]}m^{[n]}({^1h}\otimes \dots\otimes {^nh}) =  \eqqcolon \sum
\end{equation}

\begin{proof}
\begin{align*}
\Psi^{[a]}(\sum_{\sigma \in S_k} x_{\sigma(1)}\cdot\ldots\cdot x_{\sigma(k)}) &= \\
m^{[a]}\Delta^{[a]}(\sum_{\sigma \in S_k} x_{\sigma(1)}\cdot\ldots\cdot x_{\sigma(k)}) &= \\
m^{[a]}(\sum_{\sigma \in S_k} \Delta^{[a]}(x_{\sigma(1)}\cdot\ldots\cdot x_{\sigma(k)})) &= \\
m^{[a]}(\sum_{\sigma \in S_k} (x_{\sigma(1)}\otimes 1_\mathcal{H} + 1_\mathcal{H} \otimes x_{\sigma(1)}) \cdot
\ldots \cdot (x_{\sigma(k)} \otimes 1_\mathcal{H} + 1_\mathcal{H} \otimes x_{\sigma(k)}) &=
\end{align*}
\end{proof}

\begin{align*}
\Delta([x, y]) = \Delta(x\cdot y - y\cdot x) = \Delta (x\cdot y) - \Delta(y\cdot x) =
\Delta m(x \otimes y) - \Delta m(y \otimes x) =  \sum x_1 \cdot y_1 \otimes x_2 \cdot y_2 - \sum y_1
\cdot x_1 \otimes y_2 \cdot x_1 =
\end{align*}

\begin{proof} (of the lemma) \\
Proof will be as follows: \\
- we will show that if \\
- then we will show that $w \in \mathcal{H}_\nu$ iff $\lambda(l_1)\dots\lambda(l_k) \in \mathcal{H}_\nu$,
where $(l_1,\dots,l_k)$ is Lyndon factorisation of $w$.
\end{proof}

Let $|w|$ be the length of word $w$. For a word $w = a_1\dots a_{|w|}$ and permutation
$\sigma \in S_{|w|}$ let $\sigma(w) \coloneqq a_{\sigma(1)}\dots a_{\sigma(|w|)}$. \\ Let $\sim$ be
a relation on $\mathcal{X}^* \times \mathcal{X}^*$ such that for all $w, v \in \mathcal{X}^*$ such that $w = a_1\dots a_{|w|}$, $v = b_1\dots b_{|v|}$ for
$a_1,\dots, a_{|w|}, b_1, \dots, b_{|v|} \in \mathcal{X}$
\begin{equation*}
w \sim v \iff |w| = |v| = n \land \exists_{\sigma \in S_n} a_1\dots a_n = b_{\sigma(1)}\dots b_{\sigma(n)}
\end{equation*}

\begin{proof}
For the forward riffle shuffle we want to every possible permutation have probability 1 exept of identity
with probability $n-1$. Reachable permutations are the same because of form of actions (just the same)
we litterally cut that deck and put in on the top. coefficients holds, because numbers of the number of
occurences holds. Therefore its indeed the inverse riffle shuffling.
\end{proof}

\subsubsection{Alternative structure}
\indent Now we will describe an hopf algebra structure on $\mathcal{H}^{\mathrm{gd}*}$
{(definition $\gdd{V}$ for a given $V$ can be found in 2.1.1)}. \\
We define multiplication
$\Delta^* : \mathcal{H}^{\mathrm{gd}*} \otimes \gdd{\mathcal{H}} \to \gdd{\mathcal{H}}$ and
comultiplication
\text{$m^* : \gdd{\mathcal{H}} \to \gdd{\mathcal{H}} \otimes \gdd{\mathcal{H}}$} as
(for all $h_1^*, h_2^*, h^* \in \gdd{\mathcal{H}}$):
\begin{align*}
\Delta^*(h_1^* \otimes h_2^*) = (h_1^* \otimes h_2^*)\Delta, \\
m^*(h^*) = h^*m.
\end{align*}

\subsubsection{Alternative structure \smalltodo{a}}
\indent Now we will describe an alternative Hopf algebra structure on $\mathcal{H}$ - a vector space spanned
by finite words over fixed alphabet $\mathcal{X}$. It will be the
structure of $\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$ with actions induced by actions from
Hopf algebra $\mathcal{H}$ so firstly we will introduce that structure on
$\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$. But since $\mathcal{H} =
\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i$ and $\displaystyle\bigoplus^{\infty}_{i = 0}
\mathcal{H}_i \simeq \displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$ as a linear spaces, it will
work on $\mathcal{H}$ the same. We will denote a Hopf algebra with this structure
as $\mathcal{H}^*$ and call it a graded dual to $\mathcal{H}$. (but it will NOT be isomorphic to
$\mathcal{H}^*$ - vector space dual to $\mathcal{H}$). (in~\cite{Diaconis2014} that Hopf algebra is also
denoted as $\mathcal{H}^*$).  \\
It will turn out that it describes the structure of forward riffle shuffle. \\
\indent Let denote $\gdd{\mathcal{H}}$ for $\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$.
We define multiplication
$\Delta^* : \mathcal{H}^{\mathrm{gd}*} \otimes \gdd{\mathcal{H}} \to \gdd{\mathcal{H}}$ and
comultiplication
\text{$m^* : \gdd{\mathcal{H}} \to \gdd{\mathcal{H}} \otimes \gdd{\mathcal{H}}$} as
(for all $h_1^*, h_2^*, h^* \in \gdd{\mathcal{H}}$):
\begin{align*}
\Delta^*(h_1^* \otimes h_2^*) &= (h_1^* \otimes h_2^*)\Delta, \\
m^*(h^*) &= h^*m.
\end{align*}

\begin{gather*}
\sum g_1 \cdot h_1 \otimes g_2 \cdot h_2 = \\
\sum \{g_1 \cdot h_1 \otimes g_2 \cdot h_2 : g_1g_2=g \mathrm{\ and\ } h_1h_2 = h\} =\\
\sum \{ \sum\{k : g_1 \prec k \mathrm{\ and\ } h_1 = k/g_1\} \otimes
\sum\{k : g_2 \prec k \mathrm{\ and\ } h_2 = k/g_2\} : g_1g_2=g \mathrm{\ and\ } h_1h_2 = h\} = \\
\sum\{ \sum\{ m_1 \otimes m_2 : k=m_1m_2 \} : g \prec k \mathrm{\ and\ } h = k/g\} = \\
\sum\{m^*(k) : g \prec k \mathrm{\ and\ } h = k/g\} = \\
m^*\left(\sum\{k : g \prec k \mathrm{\ and\ } h = k/g\}\right) = \\
m^*\Delta^*(g \otimes h)
\end{gather*}
interlace is different
leads to a specific term

The pair of interlace and division from $m^*\Delta^*$ generates divisions of $g$ and $h$ as "that letters
that went to the prefix" and "that letters that went to the suffix" and interlaces of that prefixes and suffixes of $g$ and $h$ that are primal interlace restricted to a part of word. \\
Having
Now we will see that there is another method of introducing that structure
The
structure of $\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$ with actions induced by actions from
Hopf algebra $\mathcal{H}$ (let $\gdd{\mathcal{H}} \coloneqq \displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$): \\
multiplication
$\Delta^* : \mathcal{H}^{\mathrm{gd}*} \otimes \gdd{\mathcal{H}} \to \gdd{\mathcal{H}}$ and \\
comultiplication
\text{$m^* : \gdd{\mathcal{H}} \to \gdd{\mathcal{H}} \otimes \gdd{\mathcal{H}}$} \\
such that for all $h_1^*, h_2^*, h^* \in \gdd{\mathcal{H}}$:
\begin{align*}
\Delta^*(h_1^* \otimes h_2^*) &= (h_1^* \otimes h_2^*)\Delta, \\
m^*(h^*) &= h^*m.
\end{align*}
so firstly we will introduce that structure on
$\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$. But since $\mathcal{H} =
\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i$ and $\displaystyle\bigoplus^{\infty}_{i = 0}
\mathcal{H}_i \simeq \displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$ as a linear spaces, it will
work on $\mathcal{H}$ the same. We will denote a Hopf algebra with this structure
as $\mathcal{H}^*$ and call it a graded dual to $\mathcal{H}$. (but it will NOT be isomorphic to
$\mathcal{H}^*$ - vector space dual to $\mathcal{H}$). (in~\cite{Diaconis2014} that Hopf algebra is also
denoted as $\mathcal{H}^*$).  \\
It will turn out that it describes the structure of forward riffle shuffle. \\
\indent Let denote $\gdd{\mathcal{H}}$ for $\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$.
We define multiplication
$\Delta^* : \mathcal{H}^{\mathrm{gd}*} \otimes \gdd{\mathcal{H}} \to \gdd{\mathcal{H}}$ and
comultiplication
\text{$m^* : \gdd{\mathcal{H}} \to \gdd{\mathcal{H}} \otimes \gdd{\mathcal{H}}$} as
(for all $h_1^*, h_2^*, h^* \in \gdd{\mathcal{H}}$):
\begin{align*}
\Delta^*(h_1^* \otimes h_2^*) &= (h_1^* \otimes h_2^*)\Delta, \\
m^*(h^*) &= h^*m.
\end{align*}

so firstly we will introduce that structure on
$\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$. But since $\mathcal{H} =
\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i$ and $\displaystyle\bigoplus^{\infty}_{i = 0}
\mathcal{H}_i \simeq \displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$ as a linear spaces, it will
work on $\mathcal{H}$ the same. We will denote a Hopf algebra with this structure
as $\mathcal{H}^*$ and call it a graded dual to $\mathcal{H}$. (but it will NOT be isomorphic to
$\mathcal{H}^*$ - vector space dual to $\mathcal{H}$). (in~\cite{Diaconis2014} that Hopf algebra is also
denoted as $\mathcal{H}^*$).  \\
It will turn out that it describes the structure of forward riffle shuffle. \\

Here we will provide a more specific probabilistic interpretation of spaces and actions in $\mathcal{H}$.
We will do so by introduce algebraic structure on inverse riffle shuffle step-by-step. What we will end
up with will be eventually exactly the non-commuting algebra. Note, that what is written below are only
some of observations how structures of free-assiociative algebra and inverse riffle shuffle Markov chain
works togrther. It will not be proof that these structures are litterally the same nor that these arguments
apply  in general to all Hopf algebras and Markov chains.\\[4pt]
\begin{gather*}
\sum_{b\in\mathcal{B}}\frac{b^*\psi\psi^n \left(
\displaystyle\sum_{i = 1}^{n_0}\alpha_ib_i\right)}{s^{n+1}s_0} =
\sum_{b \in \mathcal{B}}\frac{b^*\psi\left(
\displaystyle\sum_{i = 1}^{n_0}\alpha_i\psi^n(b_i)\right)}{s^{n+1}s_0} = \\
\sum_{b\in\mathcal{B}}\frac{b^*\psi\psi^n \left(
\displaystyle\sum_{i = 1}^{n_0}\alpha_ib_i\right)}{s^{n+1}s_0} =
\sum_{b \in \mathcal{B}}\frac{b^*\psi\left(
\displaystyle\sum_{i = 1}^{n_0}\alpha_i\psi^n(b_i)\right)}{s^{n+1}s_0} = \\
\end{gather*}

First we will briefly show that $(X_0, X_1, \dots)$ are well defined by inductively showing that for all
$n \in \mathbb{N}$ there holds $\displaystyle\sum_{b \in \mathcal{B}}\mathbb{P}(X_n = b) = 1$. \\
For $n = 0$ it is straight from definition of $s_0$:
\begin{equation*}
    \sum_{b \in \mathcal{B}}\mathbb{P}(X_0 = b) = \sum_{b \in \mathcal{B}}\frac{b^*\psi^0(x_0)}{s^0s_0} =
    \sum_{b \in \mathcal{B}}\frac{b^*(x_0)}{s_0} = \frac{1}{s_0}\sum_{b \in \mathcal{B}}b^*(x_0) =
    \frac{s_0}{s_0}=1.
\end{equation*}
Suppose that for $n$ there holds $\displaystyle\sum_{b \in \mathcal{B}}\mathbb{P}(X_n = b) = 1$, then:
\begin{gather*}
    \sum_{b\in\mathcal{B}}\mathbb{P}(X_{n+1} = b) =
    \sum_{b\in\mathcal{B}}\frac{b^*\psi^{n+1}(x_0)}{s^{n+1}s_0} =
    \sum_{b\in\mathcal{B}}\frac{b^*\psi\psi^n(x_0)}{ss^ns_0} =
    \sum_{b \in\mathcal{B}}\frac{b^*\psi\left(\frac{\psi^n(x_0)}{s^{n}s_0}\right)}{s}.
\end{gather*}
Let $\ndotsm{\beta}{1}{m}$ be such that $\frac{\psi^n(x_0)}{s^ns_0} = \displaystyle\sum_{i = 1}^m\beta_ib_i$.
Then:
\begin{equation*}
    \sum_{i = 1}^m \beta_i = \sum_{b \in \mathcal{B}} b^*\left(\sum_{i=0}^m\beta_ib_i\right)=
    \sum_{b \in \mathcal{B}} b^*\left(\frac{\psi^n(x_0)}{s^ns_0}\right) = \sum_{b \in \mathcal{B}}
    \frac{b^*\psi^n(x_0)}{s^ns_0} = \sum_{b \in \mathbb{B}}\mathbb{P}(X_n = b) \overset{\mathrm{ind.}}{=}1
\end{equation*}
so:
\begin{gather*}
    \sum_{b \in\mathcal{B}}\frac{b^*\psi\left(\frac{\psi^n(x_0)}{s^{n}s_0}\right)}{s} =
    \sum_{b \in\mathcal{B}}\frac{b^*\psi\left(\displaystyle\sum_{i = 1}^m\beta_ib_i\right)}{s} =
    \frac{1}{s}\sum_{b \in \mathcal{B}}b^*\psi\left(\sum_{i=1}^m\beta_ib_i\right) = \\
    \frac{1}{s}\sum_{i=1}^m\beta_i\sum_{b \in \mathcal{B}}b^*\psi(b_i) =
    \frac{1}{s}\sum_{i=1}^m\beta_is = \frac{s}{s}\sum_{i = 0}^m\beta_i = \frac{s}{s}\cdot1=1.
\end{gather*}

\begin{gather*}
    = \\
    \frac{b_0^*(x_0)}{s_0}\prod_{i=0}^{n-1}\frac{b_{i+1}^*\psi(b_i)}\sum\frac{b^*\psi(b_n)}{s} =
    \mathbb{P}(x_0 = b_0, X_1 = b_1, \dots, x_n = b_n)\sum_{}
\end{gather*}

\todo{}
\begin{itemize}
\item dopisać commutative i cocommutative
\item dopisać przykład polinomial i ciała do coalgebry
\item dopisąć grupowy do Hopfa
\item pokazać jak wygląda coproduct w noncommuting - ważne
\item wprowadzić dualną do noncommuting - ważne
\item w rozdziale 3 wyjaśnić
\item non-commuting - Hopf square zachowuje skończone podprzestrzenie.
\item wprowadzić te podprzestrzenie
\item do łańcuch ów markowa dopisacć dokładniejsy opis gilbert shannon reeds - jakie sa prawdopodobieństwa
oraz że forward można rozumieć na dwa równoważne sposoby. To, że są do siebie odwrotne (dualne) będzie
wyprowadzone przy uzyciu algebry).
\item finer grading
\item dodać oznaczenia
\item nie dowodzić dualności!, będzie w 3.
\end{itemize}

Now we will recall two theorems from~\cite{Diaconis2014} describing eigenbases.
\begin{theorem}
bla bla eigenbasismn
\end{theorem}
\begin{theorem}
bla bla dual eigen basis
\end{theorem}
To prove them we will need a simmetrization lemma \\
bla bla\\[8pt]
\todo{Remark of no primitive generators in other algebras}
Now we can make some further observations about shuffling.\\
ble ble\\

Free assocciative, cocommutative Hopf algebra of non-commuting variables (from 2.7.1) is a model for inverse
riffle shuffling and its graded dual: commutative
Hopf algebra of non-cocommutative variables (from 2.7.3) is a model for forward riffle shuffling.
It goes as follows.
Let $\mathcal{X}$ be the finite set of all possible types of cards. Let $\nu$ be a tuple of
elements from $\mathcal{X}$ that represents our actual deck of cards (the same type of card can occur
multiple times, the order in $\nu$ should be the order in which we think cards are ordered). Now we can
take a look at a subspace $\mathcal{H}_\nu$ of vector space $\mathcal{H}$ built over $\mathcal{X}$ as
described in 2.6.2. $\mathcal{H}_\nu$ will be the subspaces spanned by words that for every type of cards
consists of exactly the same number of cards of that type as $\nu$. So the basis of $\mathcal{H}_\nu$ will
be the set
of words for every arrangement of our deck of cards. Let's name this basis $\mathcal{B}_\nu$. Note that then
$\mathcal{H}_\nu$ is finite dimensional. As was previously described, there are two ways of equipping
$\mathcal{H}$ with Hopf algebra structure. One will correspond to inverse version of riffle shuffle and
another one to the forward one. With given arrangement of cards $s \in \mathcal{B}_\nu$ applying $m\Delta$ to
$s$ yields the sum of possible outcomes after one inverse riffle shuffle while applying $\Delta^*m^*$
yields the same for forward riffle shuffle. In both cases coefficients (after normalization) are
probabilities of corresponding outcomes. \\[4pt]

Here will come an intuition why tensor products and Hopf algebras suits for describing probabilistic issues.

Let $\mathcal{X} = \{x_1, \dots x_N\}$ be our set of all possible types of cards. \\
We will denote a stack of $k$ cards containing (from top to bottom) $x_{i_1}, \dots, x_{i_k}$ simply as
$x_{i_1}\dots x_{i_k}$. \\
Imagine, that you have stack of cards $x_{i_1}\dots x_{i_k}$. After shuffling it
you can get one of finitely many stack of cards each with certain probability. We want to have some
representation of it in our structure.
For that reason we span a vector space $\mathcal{H}$, over $\mathbb{Q}$ (but can be $\mathbb{R}$ if someone
likes),
with basis $\mathcal{X}^*$ (finite words over $\mathcal{X}$, which means "all possible stacks
of cards of types from $\mathcal{X}$ including an empty stack"). \\
For all $s_1, \dots, s_n \in \mathcal{X}^*$, all $0 \leq q_1, \dots, q_n \in K$ a non-zero vector
$\displaystyle\sum^{n}_{i = 1} q_is_i$ is for all $i \in \{1, \dots, n\}$
interpreted as a state where we have a stack $s_i$ with probability $\frac{q_i}{\sum^n_{i=1} q_i}$ or
equivalently as a probabilistic measure on $\mathcal{X}^*$ with value $\frac{q_i}{\sum^n_{i=1} q_i}$ on $s_i$
for every $i \in \{1, \dots, n\}$ and $0$ elsewhere. \\
In that understanding the "+" can be read as "or". \\
We want also desribe a situation when: we have multiple stacks of cards on a table (some of them maybe
empty), there are only finitely many options how
these stacks can exactly look like and we know a probability of every option.\\
It is very natural situation during shuffling as when we for example split a stack of cards at some
random point (with known probabilities of where the split can be)
we for sure have two stacks of cards (as soon as we agree that one of them can be empty),
there are only finetely many options how exactly arrangement looks like and we know a probability of each
one.
\\ We will now focus on case when we have two decks on a table. \\
We want to deal with that matter in similar way as we done for setting "probabilistic options" to one deck
of cards. We will span a vector space with all possible arrangements of two decks as a basis.
That vector space will be $\mathcal{H} \otimes \mathcal{H}$. Now we will try to give some explanation why in
fact this is quite intuitive.  \\
For $s_1, s_2 \in \mathcal{X}^*$ let's denote $(s_1, s_2)$ as having $s_1$ on the left stack and $s_2$
on the right stack. \\
Let's make an observation that for all $s, s_1, s_2 \in \mathcal{X}^*$ situation of having arrangement
$(s_1, s)$ with probability $p$ and having arrangement $(s_2, s)$ with probability $1-p$ is
the same situation as having $s_1$ with probability $p$ or having $s_2$ with probability $1 - p$ on the left
stack and for sure having $s$ on the right stack. Making connection with our previously introduced notation
so we want to $p(s_1, s) + (1-p)(s_2, s) = (ps_1 + (1-p)s_2, s)$ (and analogously to the second
coordinate).\\
What is more, having for sure $s_1$ on the left and $s_2$ on the right with probability $p$ (and
with probability $(1 - p)$ some else arragement, let's call it $(z_1, z_2)$) gives the same probability
distribution
on possible arrangments of two decks as, having $s_1$ on the left and having $s_2$ on the
right, with probability $p$ and with probability $1-p$ having $(z_1,z_2)$).\\
This leads us to conclusion, that we also want to $p(s_1,s_2) = (ps_1, s_2)$ (and analogously to the second
coordinaate). \\
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
 Ta- daaaam! \\[4pt]
But where are that Markov chain? Where are these "subspaces preserved by $\Psi$"?

Let us denote $\mathcal{H}$ as a dual to $\mathcal{H}$ what we want to do is to see how induced
multiplication and comultiplication look like. \\
"here it come". \\
It is forward fiffle sfufle, we can check it
that corresponds to that , that to that
fold product is exactly that and that, so the coeficient matches and that is ok. \\
And then it is exactly an riffle shuffle as it is bla bla. \\

Now let us consider a vector space that is dual to $\mathcal{H}$. It is vector space of linear functions on
$\mathcal{H}$
whith basis bla bla
1$s^* : \mathcal{H} \to K$ such that for all $s \in \mathcal{X }$
We can define multiplication and comultiplication of $\mathcal{H}^*$ in the natural way.
bla bla
ble ble

We ckech, and what? That are exactly $m_F$ and $\Delta_F$ matrixes of $(F_i)_{i \geq 0}$ and $(I_i)_{i
\geq 0}$
are transpositions of each other.\\
\todo{Do poprawki, bo żal}

In the Gilbert-Shannon-Reeds model of inverse riffle shuffling there are two steps. First we are
decomposing the deck by take cards from the top of deck - one after another and putting them to the left
or to the right each with probability $\frac{1}{2}$. Secondly putting left stack on the right stack. \\
That pulling apart causes a split into two stacks, each of them can be any subset of original stack
(with preservation of order) with equal probability of each option. \\
For $s_1, s \in \mathcal{X}^*$ let denote that $s_1$ is subsequence of $s$ (a subset with preservation of
order) as $s_1 \prec s_2$. Let's denote a stack arisen from removing form $s$ its subsequence $s_1$ as
$s/s_1$. \\
Let denote that pulling apart as a $\Delta : \mathcal{H} \to \mathcal{H} \otimes \mathcal{H}$, then for all
$s \in \mathcal{X}^*$ it will give
\begin{equation*}
\Delta(s) = \sum_{\substack{s_1 \prec s \\ \land s_2 = s/s_1}}
s_1 \otimes s_2.
\end{equation*}
For putting two piles back together by placing left on the top let us write a linear map
$m : \mathcal{H} \otimes \mathcal{H} \to \mathcal{H}$ that is concatenation, which means, that
for all $s_1, s_2 \in \mathcal{X}^*$
\begin{equation*}
m(s_1 \otimes s_2) = s_1s_2.
\end{equation*}
What we just define here is exactly an algebra of non-commuting variables from example
2.3.2.\\
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
Facts about its algebraic nature are presented in that section. \\
We can observe now that Hopf-square map $\Psi^{[2]} = m\Delta$ for $\Delta$, $m$ defined as above
describes one iteration of the inverse riffle shuffle. For every $s \in \mathcal{X}^*$, $\Psi^{[2]}(s)$ is
a sum of possible arrangements of stack with, after normalisation, corresponding probabilities.
\\
For a fixed deck of $n$ cards $\nu = (\nu_1, \dots, \nu_n) \in \mathcal{X}^n$ the Markov chain of shuffling
that deck is set by $\Psi^{[2]}$ restricted to the subspace spanned by $S_\nu$ = "all $s \in \mathcal{X}^*$
that are some rearangement of $\nu$", more formally: spanned by $S_\nu$, where:
\begin{equation*}
S_\nu = \{ s = x_{i_1}\dots x_{i_n} \in \mathcal{X}^* \mid
\exists_{\sigma \in S_n} x_{\sigma(i_1)}\dots x_{\sigma(i_n)} = \nu_1\dots \nu_n. \}.
\end{equation*}
($\sigma \in S_n$ is a permutation, $S_n$ is a symmetric group of $n$ (group of
all permutations of $n$ elements)). Its equivalent to that $S_\nu = [\nu]_\SimeqSym$.\\
Then the state space of that chain is $S_\nu$. The transition matrix of that chain is exactly a matrix of
$\Psi^{[2]}$ truncated to $\mathcal{H}_\nu \coloneqq \mathrm{Lin}(S_\nu)$
(which, as we can observe is finite-dimensional and preserved by $\Psi^{[2]}$). \\[8pt]
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
For forward riffle shuffle we will be working with the same space $\mathcal{H}$ (as we still
are dealing with the same set of types of cards) but with differnt actions (as operations of "pulling apart"
and "putting together" look now different). \\
We will proove that indeed forward riffle shuffle $(F_i)_{i \geq 0}$ and inversed riffle shuffle
$(I_i)_{i \geq 0}$ are the same shuffling method
but once aplicated "forward" and once "backward". What we mean is that for fixed deck of $n$ cards
$\nu = (\nu_1, \dots, \nu_n) \in \mathcal{X}^n$, for all $s_1, s_2 \in \mathcal{H}_\nu$, all $n \geq 0$:
\begin{equation*}
\mathbb{P}\{F_{n+1} = s_2 \mid F_n = s_1\} = \mathbb{P}\{I_{n+1} = s_1 \mid I_n = s_2\}.
\end{equation*}
Which means that probability of going from state $s_1$ to state $s_2$ in one step in forward riffle shuffle
is qual to probability of going form $s_2$ to $s_1$ in one step in  inverse riffle shuffle. (which was
showed in Section 1.)\\
As remarked in Section 1. forward riffle shuffle can be defined as cutting the deck at some point with
uniform distribution on "where" ($n+1$ options for a deck of size $n$) and then putting back
two piles together in the way that \text{\textit{everyone-had-seen-at-some-point-in-the-life}}
(\textit{trrrrrrrr}) with
the same probability of every possible "\textit{trrrrrrrr}". \\

Let $\Delta_F : \mathcal{H} \to \mathcal{H} \otimes \mathcal{H}$ be an linear map for decomposition
of the deck for forward riffle shuffle, then for all $s \in \mathcal{X}^*$
\begin{equation*}
\Delta_F(s) = \sum_{s_1s_2=s} s_1 \otimes s_2.
\end{equation*}
Let $m_F : \mathcal{H} \otimes \mathcal{H} \to \mathcal{H}$ be a linear map coresponding to
\textit{trrrrrrrr}. Then for all $s_1, s_2 \in \mathcal{X}^*$:
\begin{equation*}
m_F (s_1 \otimes s_2) = \sum_{\substack{s_1 \prec s \\ s_2 = s/s_1} } s
\end{equation*}
which is sum of all possible entanglements of $s_1$ and $s_2$.\\[8pt]




\todo{MOŻE DAĆ JEDNAK TEN DOWÓD.} b
This is the end of our algebraic definitions pfuuuu...
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsection{Examples}
\subsubsection{Graded, connected Hopf algebra of polinomials}
Let $P$ be a vector space of polynomials of one variable over the field $K$ with natural grading by degree.
Note that the standard polynomial multiplication is compatible with that grading as for polynomials with
degrees $i$, $j$, their product has degree $i + j$. Connection comes from that the identity of
multiplication
is a polynomial of degree $0$ ($1_p = X^0$).  \\
$P$ can be enriched with coalgebra structure with comultiplication $\Delta$ such that for all
$n \in \mathbb{N}$:
\begin{equation*}
\Delta(X^n) = \sum^n_{i = 0} X^i \otimes X^{n-i}.
\end{equation*}
it extends linearly to the rest of $P$. \\
Counit is then $0$ for all elements with positive degree (degree $>0$). Here comes the proof: \\
Since for all $n \in \mathcal{N}$
\begin{align*}
(1_P \otimes \varepsilon )\Delta(X^n) &= X^n \otimes 1_K &\mathrm{and} \\
(1_P \otimes \varepsilon )\Delta(X^n) &= \sum^n_{i = 0} X^i \otimes \varepsilon(X^{n-i}) &\mathrm{and} \\
\sum^n_{i = 0} X^i \otimes \varepsilon(X^{n-i}) &= X^n \otimes \varepsilon(1_P) &+
\sum^{n-1}_{i=0} X^i \otimes \varepsilon(X^{n-i}) \\ &=
X^n \otimes 1_K &+ \sum^{n-1}_{i=0} X^i \otimes \varepsilon(X^{n-i})
\end{align*}
we have that for all $n \in \mathbb{N}$
\begin{equation*}
\sum^{n-1}_{i=0} X^i \otimes \varepsilon(X^{n-i}) = 0
\end{equation*}
but we also have that
\begin{equation*}
\sum^{n-1}_{i=0} X^i \otimes \varepsilon(X^{n-i}) = \sum^{n-1}_{i=0} \varepsilon(X^{n-i})X^i \otimes 1_K =
\left( \sum^{n-1}_{i=0} \varepsilon(X^{n-i})X^i \right)\otimes 1_K
\end{equation*}
Because $X_0, \dots, X_{n-1}$ are linearly independent we have that \text{$\forall_{0 \leq i \leq n-1}\
\varepsilon(X^{n - i}) = 0$}. Keeping in mind that $n$ was arbitrary, we have that for all $n \geq 1$
$\varepsilon(X^n) = 0$ and then by linearity of $\varepsilon$, that for every polynomial $p \in P$ with a
positive degree we have that $\varepsilon(p) = 0$. \\
We can now check that $P$ with that structure is a graded, connected Hopf algebra that is both commutative
and cocommutative. \\
It is a bialebra, because:
\begin{align*}
a
\end{align*}
\todo{BAM! DO ROBOTY!}

\todo{}
\begin{itemize}
\item finish
\end{itemize}

Cośtam cośtam stationary distirution. \\
\smalltodo \\
reasons of finding eigenbasis

If the reader feels lost in this section, it is recommended to read it in parallel to the section \ref{three}
where examples are provided or treat it just as a reference when formal definition will be needed.
Another reason of arranging text like that (and possibility of treating this section just as a reference),
is that for most of the time we will not be using full structure of a Hopf algebra. Nevertheless it is good
to see the full shape of what we are dealing with. \\
So now comes the full definition but we will try to explain it piece by piece. \\[8pt]

As well we can consider forward and inverse $a$-shuffles. An $a$-shuffle is a shuffle where decomposition
is made in $a$-decks, so standard shuffle is a $2$-shuffle. In inverse case we are putting cards with
probability $1/a$ on one of $a$ places and then putting piles together. This gives that we are making
$a$ subsets, each configuration of them with probability $1/a^n$. Similar analisis as in $a = 2$ case
gives that there is a probability $n^{a-1}/a^n$ for obtaining an identity permutation and a
probability $1/a^n$ for obtaining any other possible. \\
For forward case

\todo{How does it set?}

Let's take a non-commuting variables algebra from its example. Let's take $\mathcal{H}_\nu$ for some
$\nu \in \mathcal{X}^*$. Then $\Psi^{[2]}$ sets the Markov chain of inverse riffle shuffle the deck of cards
containing cards labeled by $x$s appearing in $\nu$. Chains state space is then the basis of
$\mathcal{H}_\nu$. Let's call it $\mathcal{B}_\nu$
Chains transition matrix is equal to transition matrix of $\Psi^{[2]}$ written in $\mathcal{B}_\nu$.  \\

It is grading with smallest components preserved by $\Psi$.

It will be put more
precise in the section 3. where we will present connection between Markov chains and Hopf algebras.

$\sum\!\!\!\!\!\int$, $n\!\!\!h$

\end{document}
