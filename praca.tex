\documentclass[a4paper, 12pt]{article}
\usepackage{polski}
\usepackage{amssymb}
\usepackage[polish,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{empheq}
\usepackage{xcolor}
\usepackage[pdftex,
            pdfauthor={Bartosz Sójka},
            pdftitle={Praca licencjacka},
            pdfsubject={Explanation of connection between Hopf algebras and Markov chains}]{hyperref}

%\setlength\parindent{0pt}

\newtheorem{observation}{Observation}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\newcommand{\todo}[1]{\hfill \break \textbf{\Huge \textcolor{violet}{TO DO: #1} \hfill \break}\normalsize}
\newcommand{\smalltodo}[1]{\textbf{\ \textcolor{violet}{To do}}}
\newcommand{\gdd}[1]{#1^{\mathrm{gd}*}}
\newcommand{\SimeqSym}{{\simeq_\mathrm{sym}}}
\newcommand{\onedotsn}[1]{{#1}_1, \dots, {#1}_n}
\newcommand{\ndotsm}[3]{{#1}_{#2}, \dots, {#1}_{#3}}

\title{Explanation of connection between Hopf algebras and Markov chains}
\author{Bartosz Sójka}
\date{May 2018}

\begin{document}

\thispagestyle{empty}
\begin{center}
\textbf{\large Uniwersytet Wrocławski\\
Wydział Matematyki i Informatyki\\
Instytut Matematyczny}\\
\textit{\large specjalność teoretyczna}\\
\vspace{4cm}
%\maketitle
\textbf{\textit{\large Bartosz Sójka}\\
\vspace{0.5cm}
{\Large Explanation of connection between Hopf algebras and Markov chains}}\\
\end{center}
\vspace{3cm}
{\large \hspace*{6.5cm}Praca licencjacka\\
\hspace*{6.5cm}napisana pod kierunkiem\\
\hspace*{6.5cm}prof. dr. hab. Dariusza Buraczewskiego }\\
\vfill
\begin{center}
{\large Wrocław 2018}\\
\end{center}
\newpage
\null
\thispagestyle{empty}
\newpage
\tableofcontents

\begin{abstract}

In~\cite{Diaconis2014} Persi Diaconis, Amy Pang and Arun Ram described how to use Hopf algebras to
study Markov chains. As it involves ideas from quite different branches of mathematics, it could be hard to
grasp a concept of it if someone is not familiar with them.
The point of this paper is to describe some of their results in a more step-by-step, simplified way,
so that they could be accessible to third year students after probability
and abstract algebra courses. I will focus on the example of shuffling cards by inverse riffle shuffle
method. Structure will be as follows: first there will be an introduction to both Hopf algebras and Markov
chains, then it will be explained how to describe a Markov chain with a Hopf algebra, finally I will
describe how to find left eigenbasis and right eigenbasis of Markov chain associated with riffle shuffling
using Hopf algebras.
\end{abstract}
\section{Markov chains}
\setcounter{page}{5}
Finite Markov chain is a random process on a finite set of states such that
the probability of being in some state in the moment $n+1$ depends only on the state in which one was
in the moment $n$. Now we will put this more formally.\\
Let $S = \{s_1, \dots, s_k\}$. The sequence of random variables $(X_0, X_1, \dots)$ with values in $S$
is a Markov chain with state space $S$ if for all $n \in \mathbb{N}$,
for all $s_{i_0}, s_{i_1}, \dots, s_{i_{n+1}} \in S$ such that
\begin{equation*}
\mathbb{P}(X_0 = s_{i_0}, \dots, X_n = s_{i_n}) > 0
\end{equation*}
following condition (called Markov property) holds:
\begin{equation}
\mathbb{P}(X_{n+1} = s_{i_{n+1}} \mid X_0 = s_{i_0}, \dots, X_n = s_{i_n}) =
\mathbb{P}(X_{n+1} = s_{i_{n+1}} \mid X_n = s_{i_n}).
\end{equation}
It states that for all $s_i, s_j \in S$ the probability of moving from the state
$s_i$ to the state $s_j$ is the same no matter what states $s_{i_0}, \dots, s_{i_{n-1}}$
were visited before. \\
For the Markov chain $(X_0, X_1, \dots)$ the $|S| \times |S|$ matrix
$K_{i,j} = \mathbb{P}(X_{n+1} = s_j \mid X_n = s_i)$ is called the transition matrix. We will sometimes
write $K(s_i, s_j)$ instead of $K_{i,j}$. Note that the sum of
any row is equal to 1 since it is the sum of probabilities of moving somewhere from $s_i$.
Now $K^n_{i,j}$ is the chance of moving from $s_i$ to $s_j$ in $n$ steps. \\
Markov chains can be also viewed as random walks on directed, labeled graphs, where states are vertices
and edge's label is the probability of moving from one vertex to another. \\
Card shuffling can be viewed as a Markov chain on all possible arrangements of the cards
in the deck with $K(x,y)$ equal to probability of going from arrangement $x$ to arrangement $y$ in one
shuffle.
\\ More extensive introduction can be found in~\cite{LePeWi}. \\
With the certain conditions distribution of $X_n$ converges to one distribution called stationary
distribution. Finding eigenbases and eigenvalues gives us for example how fast the Markov chain converges
to that
distribution.
%\textbf{\Huge BAM!}\normalsize
\section{Gilbert-Shannon-Reeds model of riffle shuffle}
Gilbert-Shannon-Reeds gave realistic model for forward and inverse riffle shuffle.
For forward riffle shuffle it states that we are cutting the deck with binomial distribution:
with probability $\binom{n}{k}/2^n$ for taking $k$ top cards from the deck of $n$ cards
and then riffle piles together with the same probability of every possible interlace. As riffling piles with
$k$ cards and $n-k$ cards comes to choosing a $k$ element subset of $n$ element set there is $\binom{n}{k}$
ways to do that, so then the probability of a particular interlace is equal to $1/\binom{n}{k}$.
Because of that probability of any pair of particular split and particular interlace is equal to
$\frac{\binom{n}{k}}{2^n}\frac{1}{\binom{n}{k}} = \frac{1}{2^n}$. To obtain probabilities of outcomes
we need to check how pairs of splits and interlaces corresponds to outcomes. There are two cases:
result of identity permutation can be obtained in $n$ ways by cutting the deck and then placing the top back
as it is an valid example of an interlace too. Any other permutation can by obtained in at most one way.
It is clear that if splits are the same, if interlaces are different results will be different but what is
more different splits can not lead to the same non-identity result. One of the ways to see that is to think
of cards before shuffling as they are ordered and see that one shuffling generate two increasing subsequences
in result pile. If they have different length it means that at least one card is in one and is not
in the other. If that card will not be placed on the highest possible place then it is placed below some
card from the bottom pile which do not occur in the second case. This means that all card that differs splits
have to be put on the their top-most position to generate the same permutation, what gives an identity.
For reverse riffle shuffle it states that we are sequentially putting cards from the bottom of the pile to
the or to the right with probability $\frac{1}{2}$. Then we place left pile on the top of the right pile.
Again there are $n$ ways (pairs of division and concatenation) of obtaining an identity permutation -
for every $0 \leq k \leq n$ by putting $k$ cards on the right and then $n-k$ cards on the left.
Again every other permutation can be obtained in at most one way and, again, every pair of division and
concatenation have the same probability. To see that every non-identity permutation can be obtained in
at most one way we can think of division as of choosing a subsequence of the deck and placing it to the left.
Now suppose we have chosen two different subsets. Let $c$ be a card that is in one of them but not in the
other. If that card is not the most bottom of the chosen subset results will be different as in one case
it will be between some cards from the left pile and in second not. The same goes with not the top one
from non-chosen set. It gives that for different subsets chooses to give the same result the cards that
differ them have to be top ones from the unselected and the most bottom ones from chosen what means
giving the identity permutation as we then just cut deck into two peaces in different ways. \\
Conclusion is that in this model both for forward and inverse riffle shuffle probability of obtaining
an identity permutation is equal to $n/2^n$ and for any other possible $1/2^n$. \\
We can also consider forward and inverse $a$-shuffles. An $a$-shuffle is a shuffle where decomposition
is made in $a$-decks, so standard shuffle is a $2$-shuffle. In the forward case we are splitting the deck
into $a$ piles and then riffling them together with the same probability of every scenario.
In inverse case we are putting cards with probability $1/a$ on one of $a$ places and then putting piles
together. More specific analysis of $a$ shuffles can be done with Hopf algebras.
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\section{Hopf algebras}
Now I will give a full definition of a Hopf algebra.
Reader can as well skip this section and treat it like a reference when formal definition or explanation
will be needed. Although it is quite long I decided to put it in a consistent fragment, due to belief
that thanks to that it will better serve that purpose.
Another reason is that for most of the time we will not be using full structure of a Hopf algebra,
nevertheless it is good
to see the full shape of what we are dealing with. \\
\subsection{Preliminaries}
\subsubsection{Rotational remarks}
\indent \textbf{Remark. }Let $K$ be a field. In the following section $k$, if not stated otherwise, will
denote an arbitrary element of this field. If not stated otherwise, all vector spaces will be over $K$
and all tensor products will be taken over $K$. Note that when we want to present a field
multiplication from $K$ as a linear map $K \otimes K \to K$ it will be denoted as $m_K$. As it is then an
isomorphism let $\Delta_K \coloneqq m_K^{-1}$. The $1$ from $K$ will be denoted as $1_K$.\\[8pt]
\textbf{Remark. } Let $U, V, W, Z$ be vector spaces over field $K$.
We will use notation
$\varphi \otimes \psi:U \otimes V \to W \otimes Z$ which, for
$\varphi$, $\psi$ such that $\varphi : U \to W$, $\psi : V \to Z$, means a linear map that
for all $u \in U$, $v \in V$ satisfies:
\begin{equation*}
(\varphi \otimes \psi)(u \otimes v) = \varphi(u) \otimes \psi(v).
\end{equation*}
Because of linearity, for elements of shape $\displaystyle\sum^n_{i=1} u_i \otimes v_i$ it will take form:
\begin{equation*}
(\varphi \otimes \psi)(\sum^n_{i = 1} u \otimes v) = \sum^n_{i = 1}\varphi(u) \otimes \psi(v).
\end{equation*}
$I$, if not stated otherwise, will be an identity in the adequate space. \\
$T$, if not stated otherwise, will
be the twist map $T:V \otimes W \to W\otimes V$, which is linear map such that for any $v \otimes w
\in V \otimes W$
\begin{equation*}
T(v \otimes w) = w\otimes v.
\end{equation*}
For an n-tensor power $\overbrace{V \otimes \dots \otimes V}^{n\ times}$  of a vector space $V$ we will
sometimes write $V^{\otimes n}$.\\
Throughout this paper, when there will be no risk
of confusion, we will omit the "$\circ$" symbol of composition of functions and we will write
$\varphi \psi (x)$ instead of $(\varphi \circ \psi)(x)$. \\
\textbf{Dual spaces} \\
We will use standard notation for dual spaces: \\
For a vector space $V$ over a field $K$ we will write $V^*$ for a vector space dual to $V$ -
a vector space of all linear functions from $V$ to $K$.
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Tensor products}
%
First we will introduce tensor product of the vector spaces. Let $V, W$ be vector spaces over the field $K$
with basises , respectively, $\mathcal{B}$, $\mathcal{C}$. \\
We define tensor product $V \otimes W$ as a linear space with basis
\begin{equation*}
\{v \otimes w : v \in \mathcal{B}, w \in \mathcal{C} \}.
\end{equation*}

For arbitrary elements $v \in V$, $w \in W$ such that $v =\sum_{b_i\in \mathcal{B}}
\beta_1 b_i$ and $w = \sum_{c_i \in \mathcal{C}} \gamma_i c_i$ we will interpret
expresion $v \otimes w$ as:
\begin{equation*}
        v \otimes w \coloneqq \sum_{b_i \in \mathcal{B}}\sum_{c_j \in \mathcal{C}} \beta_i\gamma_j
        b_i\otimes c_j
\end{equation*}

\begin{observation}
If $V$ and $W$ are finite dimensional and $\mathrm{ dim}(V)=n$, \text{$\mathrm{ dim}(W)=m$}, then
$\mathrm{dim}(V \otimes W)=nm$.
\end{observation}

\begin{observation}\label{observation:2}
If $V$ is a vector space over $K$, then all elements of $K \otimes V$ ($V \otimes K$) can be expressed in
form $1 \otimes v$ ($v \otimes 1$) and there are natural isomorphisms $m_L : K \otimes V \to V$,
($m_R : V \otimes K \to V$) given by
\begin{align*}
m_L(k \otimes v) &= kv, \\
m_R(v \otimes k) &= kv.
\end{align*}
\end{observation}
\begin{proof}
An arbitrary element of $K \otimes V$ has form $\displaystyle\sum^n_{i=1}k_i \otimes v_i$ but
\begin{equation*}
\sum^n_{i=1}k_i \otimes v_i = \sum^n_{i=1} 1 \otimes k_iv_i = 1 \otimes \sum^n_{i=1}k_iv_i.
\end{equation*}
$m_L$ is linear (left for the reader) and is bijective because for all $v, v_1, v_2 \in V$
\begin{equation*}
\varphi(1 \otimes v) = v
\end{equation*}
and
\begin{align*}
1 \otimes v_1 = 1 \otimes v_2 &\iff 1 \otimes v_1 - 1 \otimes v_2 = 0 \iff \\
1 \otimes (v_1 - v_2) = 0 &\iff v_1 -v_2 = 0 \iff v_1 = v_2.
\end{align*}
The proof for $V \otimes K$ and $m_R$ is analogous.\end{proof}
 In the later sections we will use notations of
$m_L$ and $m_R$ for those isomorphism for any space. \\
\textbf{Remark. } In a special case when $V = W = K$ the natural isomorphisms described above
take form of $m_K : K \otimes K \to K$ that for all $k_1, k_2\in K$
\text{$m_K(k_1 \otimes k_2) = k_1k_2$}. This isomorphism of $K \otimes K$ and $K$ is just a field
multiplication from $K$.
\begin{observation}
For vector spaces $U, V, W$ over the field $K$ there is a natural isomorphism between
$(U \otimes V) \otimes W$ and $U \otimes (V \otimes W)$ therefore there is no ambiguity in writing
$U \otimes V \otimes W$ or a product of any greater number of vector spaces in that way. (As well as denoting
elements of their basises in the form "$u \otimes v \otimes w$".)
\end{observation}
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////

\subsection{Algebras}
\begin{definition}
A \textbf{$\textbf{K}$-algebra} is a vector space $\mathcal{A}$ with additional
associative, linear operation
\text{$m:\mathcal{A} \otimes \mathcal{A} \to \mathcal{A}$} called multiplication and a linear map
\text{$u:K\to \mathcal{A}$} called unit such that for all $a \in \mathcal{A}$
\begin{equation*}
m(u(1_K) \otimes a) = m(a \otimes u(1_K)) = a.
\end{equation*}
\end{definition}
\textbf{Explanation. } Operation $m$ defines on $\mathcal{A}$ a structure of a unitary ring by setting the
ring multiplication (let it be denoted as "$\cdot$") as $a \cdot b = m(a \otimes b)$. The identity element
of that ring multiplication is then $u(1_K)$. (We will be calling $u(1)$ also an identity element of
multiplication $m$ in K-algebra $\mathcal{A}$ or the $1$ in the $\mathcal{A}$ and
denote it as $1_\mathcal{A}$)\\
\textit{Proof.} The fact that $m$ is associative means that for all $a_1, a_2, a_3 \in \mathcal{A}$
\begin{equation*}
m(m(a_1 \otimes a_2) \otimes a_3) = m(a_1 \otimes m(a_2 \otimes a_3)).
\end{equation*}
That implies that
\begin{equation*}
(a\cdot b) \cdot c = m(m(a \otimes b) \otimes c) = m(a \otimes m(b \otimes c)) = a \cdot (b \cdot c).
\end{equation*}
So "$\cdot$" is proper ring multiplication. Recalling the definition of $u$, we can write that for all
$a \in \mathcal{A}$
\begin{equation*}
u(1_K) \cdot a = a \cdot u(1_K) = a
\end{equation*}
So indeed it is an identity
element of that ring. As $u$ is linear map it can be seen as natural insertion of a field $K$ into an
algebra $\mathcal{A}$ that maps $1_K$ to $1_\mathcal{A}$ ($1$ from the $K$ to the identity element of
multiplication in $\mathcal{A}$) and extends linearly. Given that we can observe
that for all $a \in \mathcal{A}$, all $k \in K$, $a$ multiplicated by $u(k)$ (no matter if from the left
or right) is exactly the $ka$ (an element of vector space $\mathcal{A}$).
So we can think about $u[K]$ as a copy of $K$ in $\mathcal{A}$ that acts on $\mathcal{A}$ just like $K$.\\
\indent  Because of associativity we can define $m^{[3]} : \mathcal{A}^{\otimes 3} \to \mathcal{A}$ as
\begin{equation*}
m^{[3]} \coloneqq m(m\otimes I)
\end{equation*}
and for all $a_1, a_2, a_3 \in \mathcal{A}$ write
\begin{equation*}
m^{[3]}(a_1 \otimes a_2 \otimes a_3) = a_1 \cdot a_2 \cdot a_3
\end{equation*}
with no ambiguity.
And further: \\
\indent Let $A$ be an algebra with multiplication $m$ and unit $u$. We will recurrsivly define a
sequence of maps
$(m^{[n]})_{n \geq 2}$, such that $m^{[n]} : \underbrace{\mathcal{A} \otimes \dots \otimes
\mathcal{A} }_{n\ times} \to \mathcal{A} $
as follows:
\begin{align*}
m^{[2]} &\coloneqq m, \\
m^{[n]} &\coloneqq m^{[n-1]}(m \otimes \underbrace{I \otimes \dots \otimes I}_{n-2 \mathrm{\ times}})
\end{align*}
which is a multiplication of all factors together. \\
Because of that, for all $a_1, \dots, a_n \in \mathcal{A} $ we can write
\begin{equation*}
m^{[n]}(a_1 \otimes \dots \otimes a_n) = a_1 \cdot \ldots \cdot a_n.
\end{equation*}
\textbf{Remark. } An algebra $\mathcal{A} $ is said to be commutative iff for all $a_1, a_2 \in \mathcal{A}$
\begin{equation*}
m(a_1 \otimes a_2 ) = m(a_2 \otimes a_1).
\end{equation*}
\indent \textbf{Remark. } Later in the text we will still be using "$\cdot$" as a symbol for an algebra
multiplication in an algebra of our interest.
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsection{Coalgebras}
\begin{definition}
A \textbf{$\textbf{K}$-coalgebra} is a vector space $\mathcal{C}$
with additional coassociative, linear operation
\text{$\Delta : \mathcal{C} \to \mathcal{C} \otimes \mathcal{C}$} called comultiplication and
a linear map \text{$\varepsilon : \mathcal{C} \to K$} called counit such that for all
$a \in \mathcal{C}$
\begin{align*}
(\varepsilon \otimes I)\Delta(a) &= 1 \otimes a \mathrm{\ and} \\
(I \otimes \varepsilon)\Delta(a) &= a \otimes 1.
\end{align*}
\end{definition}
Note that properties of a unit from a $K$-algebra can also be written in that manner as:
\begin{align*}
m(u \otimes I)(1_K \otimes a) &= a \mathrm{\ and}\\
m(I \otimes u)(a \otimes 1_K) &= a
\end{align*}
means exactly what was in the definition of $u$. \\[8pt]
\textbf{Explanation.} We will introduce a notation called Sweedler notation [Swe69] which will be
very useful
for writing coproducts. As for all $a \in \mathcal{C}$ we have
\text{$\Delta(a) = \displaystyle\sum^n_{i=1}a_{1,i} \otimes a_{2,i}$}, we will write
\begin{equation*}
\Delta(a) = \displaystyle\sum a_1 \otimes a_2.
\end{equation*}
This notation suppresses the index "$i$". Somewhere there can be also encountered an interjaced
notation \text{$\Delta(a) = \displaystyle\sum_{(a)}a_{(1)}\otimes a_{(2)}$}. \\
In many cases comultiplication can be seen as a sum of possible decomposition of an element into
elements "smaller" in some sense.
For example, later it will come out that comultiplication is exactly the operation that
models the process of cutting the deck of cards into pieces in riffle shuffle. In examples that we will
work with (graded, connected Hopf algebras), comultiplication will represent some kind of natural
decomposition in the more general way. What it means in the strict sense will be presented in
definition \ref{bialgebras} when we will be introducing graded bialgebras. \\
Examples.
For a linear space of polynomials we can define comultiplication as:
\[
    \Delta(X^n) = \sum_{i=0}^n \binom{n}{i} X^i \otimes X^{n-i}
\]
The coassociativity of $\Delta$ means that $(\Delta \otimes I)\Delta = (I \otimes \Delta)\Delta$.
In Sweedler notation it can be written as
\begin{equation*}
\forall_{a \in \mathcal{C}}\ \sum\Delta(a_1) \otimes a_2 = \sum a_1 \otimes \Delta(a_2)
\end{equation*}
or in a more expanded form as
\begin{equation}\label{swe1}
\forall_{a\in \mathcal{C}}\ \sum\ {a_1}_1 \otimes {a_1}_2 \otimes a_2 = \sum a_1 \otimes {a_2}_1
\otimes {a_2}_2.
\end{equation}
Because of these equalities, terms from (\ref{swe1}) can be written as
\text{$\displaystyle\sum a_1 \otimes a_2 \otimes a_3$} without ambiguity. \\
We can also define
\begin{equation*}
\Delta^{[3]} \coloneqq (\Delta \otimes I)\Delta
\end{equation*}
Now, for all $a \in \mathcal{C}$ there will be an equality
\begin{equation*}
\Delta^{[3]}(a) = \sum a_1 \otimes a_2 \otimes a_3
\end{equation*}
which can be viewed as a sum of possible decompositions of $a$ into three parts.
In this point of view we can say that coassociativity of $\Delta$ means that $\Delta$ represents
decomposition such that result is the same, no matter which set of parts ($a_1$ or $a_2$) have been taken
in the second iteration. It can become more
clear with an introduction of the examples that are presented in \ref{polinomials} and later. \\
Now we will take it a step further: \\[8pt]
\indent Let $\mathcal{C} $ be a coalgebra with comultiplication $\Delta$ and counit $\varepsilon$.
We will recurrently define a sequence of maps $(\Delta^{[n]})_{n \geq 2}$, such that
$\Delta^{[n]} : \mathcal{C}  \to \underbrace{\mathcal{C}  \otimes \dots \otimes
\mathcal{C} }_{n \mathrm{\ times}}$ as follows:
\begin{align*}
\Delta^{[2]} &\coloneqq \Delta, \\
\Delta^{[n]} &\coloneqq (\Delta \otimes
\underbrace{I \otimes \dots \otimes I}_{n - 2 \mathrm{\ times}})\Delta_{n-1}.
\end{align*}
Which can be seen as composed iterations of $\Delta$. \\
By induction it can be proved that for all $n \geq 3$, $i \in \{1, \dots, n-2\}$,
\text{$m \in \{0, \dots, n-i-1\}$} we have
\begin{equation*}
\Delta^{[n]} = (\underbrace{I \otimes \dots \otimes I}_{m\ \mathrm{times}} \otimes
\Delta^{[i]} \otimes \underbrace{I \otimes \dots \otimes I}_{n-i-1-m \mathrm{\ times}})\Delta^{[n-i]},
\end{equation*}
The proof can be found in ~\cite{DNR} (Proposition 1.1.7 and Lemma 1.1.10, sites 5-7). Note that the
notation is slightly different there - it is $\Delta_1 \coloneqq \Delta$ not $\Delta^{[2]} \coloneqq
 \Delta$. \\
This formula is a generalization of coassociativity. It means that $\Delta^{[n]}$ is coproduct where
$\Delta$ is applied $n-1$ times to any tensor factor at each stage. Thanks to that we can write
\begin{equation*}
\Delta^{[n]}(a) = \sum a_1 \otimes \dots \otimes a_n
\end{equation*}
with no ambiguity. \\
Interpretation is an extension of that described in the previous paragraph for $n = 2$. Now we are just
decomposing $a$ into $n$ parts and result do not depend on which factors we are
applying $\Delta$ at each stage.  \\[8pt]
The counit property written in Sweedler notation takes form
\begin{align*}
\sum\varepsilon(a_1) \otimes a_2 &= 1 \otimes a, \\
\sum a_1 \otimes \varepsilon(a_2) &= a \otimes 1.
\end{align*}
Applying isomorphisms $m_L$ and $m_R$ defined in
\hyperref[observation:5]{Observation 5.} on both sides respectively we get
\begin{align*}
\sum\varepsilon(a_1)a_2 &= a, \\
\sum a_1\varepsilon(a_2) &= a.
\end{align*}
\textbf{Remark. } A coalgebra $\mathcal{C} $ is said to be cocommutative iff for all $c \in \mathcal{C} $
\begin{equation*}
\sum c_1 \otimes c_2 = \sum c_2 \otimes c_1.
\end{equation*}
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsection{Bialgebras}
\begin{definition}
A \textbf{$\textbf{K}$-bialgebra} is vector space $\mathcal{B}$ with both an algebra structure
$(\mathcal{B}, m, u)$ and a coalgebra structure $(\mathcal{B}, \Delta, \varepsilon)$ such that $m$, $u$
are morphisms of coalgebras and $\Delta$, $\varepsilon$ are morphisms of algebras.
\end{definition}
\textbf{Explanation. } In fact, for a given vector space $\mathcal{B}$ with both an algebra structure
$(\mathcal{B}, m, u)$ and a coalgebra structure $(\mathcal{B}, \Delta, \varepsilon)$, the fact that
$m$ and $u$ are morphisms of coalgebras is equivalent to the fact that $\Delta$ and $\varepsilon$ are
morphisms of
algebras and both are equivalent to conjuction of following conditions:
\begin{align*}
\Delta m &= (m\otimes m)(I \otimes T \otimes I)(\Delta \otimes \Delta), \\
\varepsilon m &= m_K(\varepsilon \otimes \varepsilon), \\
\Delta u &= (u \otimes u)\Delta_K, \\
\varepsilon u &= I.
\end{align*}
They can also be written as: for all $g, h \in \mathcal{B}$, all $k \in K$
\begin{align*}
\sum (g \cdot h)_1 \otimes (g \cdot h)_2 &= \sum g_1 \cdot h_1 \otimes g_2 \cdot h_2, \\
\varepsilon(g \cdot h) &= \varepsilon(g)\varepsilon(h), \\
\sum (1_\mathcal{B})_1 \otimes (1_\mathcal{B})_2 &= 1_\mathcal{B} \otimes 1_\mathcal{B}, \\
\varepsilon (1_\mathcal{B}) &= 1_K.
\end{align*}
or as: for all $g, h \in \mathcal{B}$, all $k \in K$
\begin{align*}
\Delta(g \cdot h) &= \sum g_1 \cdot h_1 \otimes g_2 \cdot h_2, \\
\varepsilon(g \cdot h) &= \varepsilon(g)\varepsilon(h), \\
\Delta (1_\mathcal{B}) &= 1_\mathcal{B} \otimes 1_\mathcal{B}, \\
\varepsilon (1_\mathcal{B}) &= 1_K.
\end{align*}
\textbf{Remark. } Note that for the condition
$\Delta m = (m\otimes m)(I \otimes T \otimes I)(\Delta \otimes \Delta)$
we need the map $(I \otimes T \otimes I)$, because without it, the right side will be equal to
$(m \otimes m)(\Delta \otimes \Delta)$ which, when applied to vector $g \otimes h$ yields
$\displaystyle\sum g_1 \cdot g_2 \otimes h_1 \cdot h_2$ not $\sum g_1 \cdot h_1 \otimes g_2 \cdot h_2$
and we want comultiplication and multiplication to be done componentwise. Definition with
one $T$ is enough for all powers of $m$ and $\Delta$ as stated in the following remark:\\
\textbf{Remark. } It can be proven by induction that for all ${^1h}, \dots, {^nh} \in \mathcal{B}$
\begin{equation}
\Delta^{[m]}m^{[n]}({^1h} \otimes \dots \otimes {^nh}) = \sum {^1h}_1 \cdot \ldots \cdot {^nh}_1
\otimes \dots \otimes {^1h}_m\cdot  \ldots \cdot {^nh}_m.
\end{equation}
\begin{proof}
Left to the reader.
\end{proof}
To simplify the notation, we will write a symbol for algebra multiplication also for componentwise
multiplication, \\ so for all ${^1h}_1, \dots, {^1h}_m, \dots, {^nh}_1, \dots , {^nh}_m \in \mathcal{B}$:
\begin{equation}
({^1h}_1\otimes \dots \otimes {^1h}_m)\cdot \ldots \cdot ({^nh}_1 \otimes \dots \otimes {^nh}_m) \coloneqq
{^1h}_1 \cdot \ldots \cdot {^nh}_1 \otimes \dots \otimes {^1h}_m\cdot  \ldots \cdot {^nh}_m.
\end{equation}
\begin{definition}
Element $b$ of a bialgebra $\mathcal{B}$ is said to be \textbf{primitive} iff
\begin{equation*}
\Delta(b) = 1_\mathcal{B} \otimes b + b \otimes 1_\mathcal{B}
\end{equation*}
\end{definition}
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\begin{definition}
For a bialgebra $\mathcal{B}$ we define a \textbf{Hopf-square} map
$\Psi^{[2]} : \mathcal{B} \to \mathcal{B}$ as $\Psi^{[2]} \coloneqq m\Delta$.
\end{definition}
\textbf{Comment. } This will be a very important function in this paper. It will be this function, that will
set a structure
of a Markov chain on a Hopf algebra. In Hopf algebras that we will
use for modelling Markov chains, the Hopf square map will preserve some of those algebras'
(viewed as a vector space) finite dimensional subspaces. Basises of these preserved
subspaces can be then treated as spaces of states (aces of spades, haha)
of our associated Markov chains. Note that one Hopf algebra can set a structure of many Markov chains,
each one having a basis of algebra's finite dimensional subspace preserved by $\Psi^{[2]}$ as its (chains)
space of states.
What's more, the matrix of $\Psi^{[2]}$ (viewed as a transformation of some fixed, finite-dimensional
subspace of algebra)
written in the base $\mathcal{B}$ of that subspace will be exactly a transition matrix
$K_{i,j}$ of associated Markov chain on that basis. Finding eigenbasis of $K_{i,j}$ is then expressed as
finding eigenvectors of $\Psi^{[2]}$. Later it will be put more carefully and precisely. \\
It will have a natural interpretation as "pulling apart" and then "putting pieces together", for
example splitting the deck of cards and then shuffling it. \\[4pt]
We also define higher power maps for $n \geq 2$:
\begin{equation*}
\Psi^{[n]} \coloneqq m^{[n]}\Delta^{[n]}.
\end{equation*}
Hopf-square in Sweedler notation looks like this:
\begin{equation*}
\Psi^{[n]}(a) = \sum a_1 \cdot \ldots \cdot a_n.
\end{equation*}
For $\Psi^{[2]}$ we will sometimes simply write $\Psi$.
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsubsection{Convolution}
\begin{definition}
Let $(C, \Delta, \varepsilon)$ be a coalgebra and $(A, M, u)$ an algebra. On the set $Hom(C, A)$ we define
an algebra structure in which the multiplication, denoted by $*$, is given as follows: if
$f, g \in Hom(C, A)$, then
\begin{equation*}
f*g \coloneqq m(f \otimes g)\Delta
\end{equation*}
we call $*$ the \textbf{convolution} product.
\end{definition}
It can be also written as: for any $c \in C$, any $f, g \in Hom(C, A)$
\begin{equation*}
(f*g)(c) = \sum f(c_1) \cdot g(c_2)
\end{equation*}
The multiplication defined above is associative, since for $f, g, h \in Hom(C, A)$ and
$c \in C$ we have
\begin{align*}
((f*g)*h)(c) &= \sum(f*g)(c_1)\cdot h(c_2) \\
&= \sum f(c_1) \cdot g(c_2) \cdot h(c_3) \\
&= \sum f(c_1) \cdot (g*h)(c_2) \\
&= (f*(g*h))(c).
\end{align*}
The identity element of the algebra $Hom(C, A)$ is $u\varepsilon \in Hom(C, A)$ since
\begin{align*}
(f * u\varepsilon)(c) &= \sum f(c_1) \cdot u\varepsilon(c_2) \\
&= \sum f(c_1) \cdot \varepsilon(c_2)1_A \\
&= \sum f(c_1)\varepsilon(c_2) \cdot 1_A \\
&= \left(\sum f(c_1)\varepsilon(c_2)\right)\cdot 1_A \\
&= f(c) \cdot 1_A = f(c)
\end{align*}
hence $f * u\varepsilon = f$. Similarly, $u\varepsilon * f = f$. \\
Let us note that if $A = K$, then $*$ is the convolution product defined on the dual algebra of the
coalgebra $C$. This is why in the case $A$ is an arbitrary algebra we will also call $*$ the convolution
product. \\[8pt]
\indent For a bialgebra $\mathcal{B}$ we denote $\mathcal{B}^A$, $\mathcal{B}^C$ as, respectively,
the underlying algebra and coalgebra structure. We can define algebra structure on
$Hom(\mathcal{B}^C, \mathcal{B}^A)$ as above. Note that the identity map $I : \mathcal{B} \to \mathcal{B}$
is an
element of $Hom(\mathcal{B}^C, \mathcal{B}^A)$ but it is not the identity element of its algebra structure
with convolution product. The $u\varepsilon$ is that identity element.
\begin{definition}
Let $\mathcal{H}$ be a bialgebra. A linear map $S \in Hom(\mathcal{H}^C, \mathcal{H}^A)$ is called an
\textbf{antipode} of the bialgebra $\mathcal{H}$ if $S$ is the inverse of the identity map
$I : \mathcal{H} \to \mathcal{H}$ with respect to the convolution product in
$Hom(\mathcal{H}^C, \mathcal{H}^A)$
\end{definition}
The fact that $S \in Hom(\mathcal{H}^C, \mathcal{H}^A)$ is an antipode is written as
\begin{equation*}
S * I = I * S = u\varepsilon.
\end{equation*}
and using Sweedler notation as:
\begin{equation*}
\forall_{h \in \mathcal{H}} \sum S(h_1) \cdot h_2 = \sum h_1 \cdot S(h_2) = \varepsilon(h)1_\mathcal{H}.
\end{equation*}
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsection{Hopf algebras}
\begin{definition}
A bialgebra having an antipode is called a \textbf{Hopf algebra}.
\end{definition}
\begin{definition}\label{bialgebras}
A \textbf{graded bialgebra} is a graded vector space
$\mathcal{H} = \displaystyle\bigoplus^{\infty}_{i=0}\mathcal{H}_i$ with a bialgebra structure that is
compatible with the grading.
\end{definition}
\textbf{Explanation. } A bialgebra structure is compatible with grading iff for all $i, j \in \mathbb{N}$:
\begin{align*}
m[\mathcal{H}_i \otimes \mathcal{H}_j] &\subseteq \mathcal{H}_{i+j} \mathrm{\ and} \\
\Delta[H_n] &\subseteq \bigoplus^{n}_{i = 0} \mathcal{H}_i \otimes \mathcal{H}_{n-i}.
\end{align*}
\indent Now decomposition can be viewed as representing an element as the sum of pairs of lower-degree
("smaller") elements. \\
\indent We can observe that
\begin{align*}
\Psi^{[2]}[\mathcal{H}_n] &= m\Delta[\mathcal{H}_n] \subseteq m[\bigoplus^n_{i = 0} \mathcal{H}_i \otimes
\mathcal{H}_{n - i}] \\ &= \bigoplus^n_{i = 0} m[\mathcal{H}_i \otimes \mathcal{H}_{n - i}] \subseteq
\bigoplus^n_{i = 0} \mathcal{H}_n = \mathcal{H}_n,
\end{align*}
hence Hopf square $\Psi^{[2]}$ preserves grading
(in the sense that $\Psi^{[2]}[\mathcal{H}_n] \subseteq \mathcal{H}_n$).
\begin{definition}
A graded bialgebra $\mathcal{H} = \displaystyle\bigoplus^{\infty}_{i=0}\mathcal{H}_i$ is
\textbf{connected} iff $\mathcal{H}_0$ is one-dimensional subspace spanned by $1_\mathcal{H}$.
\end{definition}
\noindent \textbf{Explanation. } Equivalently we can say that a graded bialgebra
$\mathcal{H} = \displaystyle\bigoplus^{\infty}_{i=0}\mathcal{H}_i$ is
connected iff $\mathcal{H}_0 = u[K]$ for $u$ - unit in $\mathcal{H}$ treated as a $K$-algebra.
\begin{theorem}
Any graded, connected bialgebra is a Hopf algebra with antipode:
\begin{equation*}
S = \sum_{k \geq 0} (u\varepsilon - I)^{*k}.
\end{equation*}
\end{theorem}
\begin{proof}
    Left to a curious reader.
\end{proof}
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsection{Examples}\label{polinomials}
\subsubsection{Graded, connected Hopf algebra of polynomials}
Let $P$ be a vector space of polynomials of one variable over the field $K$ with natural grading by degree.
Note that the standard polynomial multiplication is compatible with that grading as for polynomials with
degrees $i$, $j$, their product has degree $i + j$. Connection comes from that the identity of
multiplication
is a polynomial of degree $0$ ($1_p = X^0$).  \\
$P$ can be enriched with coalgebra structure with comultiplication $\Delta$ such that for all
$n \in \mathbb{N}$:
\begin{equation*}
\Delta(X^n) = \sum^n_{i = 0} \binom{n}{i} X^i \otimes X^{n-i}.
\end{equation*}
it extends linearly to the rest of $P$. \\
Counit is then $1_K$ for $X^0$ since $\varepsilon u = I$ and
$0$ for all elements with positive degree (degree $>0$). Here comes the proof: \\
Since for all $n \in \mathcal{N}$
\begin{align*}
(I \otimes \varepsilon )\Delta(X^n) &= X^n \otimes 1_K &\mathrm{and} \\
(I \otimes \varepsilon )\Delta(X^n) &= \sum^n_{i = 0}\binom{n}{i} X^i \otimes \varepsilon(X^{n-i}) &\mathrm{and} \\
\sum^n_{i = 0}\binom{n}{i} X^i \otimes \varepsilon(X^{n-i}) &= X^n \otimes \varepsilon(1_P) +
\sum^{n-1}_{i=0}\binom{n}{i} X^i \otimes \varepsilon(X^{n-i}) \\ &=
X^n \otimes \, 1_K \ \ \ + \sum^{n-1}_{i=0}\binom{n}{i} X^i \otimes \varepsilon(X^{n-i})
\end{align*}
we have that for all $n \in \mathbb{N}$
\begin{equation*}
\sum^{n-1}_{i=0}\binom{n}{i} X^i \otimes \varepsilon(X^{n-i}) = 0
\end{equation*}
but we also have that
\begin{equation*}
\sum^{n-1}_{i=0}\binom{n}{i} X^i \otimes \varepsilon(X^{n-i}) = \sum^{n-1}_{i=0}\binom{n}{i}
 \varepsilon(X^{n-i})X^i \otimes 1_K =
\left( \sum^{n-1}_{i=0}\binom{n}{i} \varepsilon(X^{n-i})X^i \right)\otimes 1_K
\end{equation*}
Because $X^0, \dots, X^{n-1}$ are linearly independent we have that \text{$\forall_{0 \leq i \leq n-1}\
\varepsilon(X^{n - i}) = 0$}. Keeping in mind that $n$ was arbitrary, we have that for all $n \geq 1$
$\varepsilon(X^n) = 0$ and then by linearity of $\varepsilon$, that for every polynomial $p \in P$ with a
positive degree we have that $\varepsilon(p) = 0$. $\square$\\
We can now check that $P$ with that structure is a graded, connected Hopf algebra that is both commutative
and cocommutative. \\
It is a bialebra, because:
\begin{enumerate}
\item for all $n, m \in \mathbb{N}$
\begin{align*}
\Delta m(X^n \otimes X^m) = \Delta(X^{n+m}) = \sum_{k=0}^{n+m}\binom{n+m}{k}X^i \otimes X^{n+m - k}
\end{align*}
and on the other hand:
\begin{gather*}
    (m\otimes m)(I \otimes T \otimes I)(\Delta \otimes \Delta) (X^n \otimes X^m) = \\
    (m \otimes m)(I \otimes T \otimes I)\left(\left(\sum_{i=0}^n\binom{n}{i}X^i \otimes X^{n-i}\right)
    \otimes\left(\sum_{j=0}^m\binom{m}{j}X^j \otimes X^{m-i}\right)\right) = \\
    (m \otimes m)(I \otimes T \otimes I)\left(\sum_{\substack{0\leq i \leq n \\ 0 \leq j \leq m}}
    \binom{n}{i}\binom{m}{j}X^i\otimes X^{n-i} \otimes X^j \otimes X^{m-j}\right) = \\
    (m \otimes m)\left(\sum_{\substack{0 \leq i \leq n \\ 0 \leq j \leq m}}
    \binom{n}{i}\binom{m}{j}X^i \otimes X^j \otimes X^{n-i} \otimes X^{m-j}\right) = \\
    \sum_{\substack{0 \leq i \leq n \\ 0 \leq j \leq m}}\binom{n}{i}\binom{m}{j}X^{i+j}\otimes X^{n+m-i-j}
    = \\
    \sum_{k=0}^{n+m}\sum_{i=0}^k \binom{n}{i}\binom{m}{k-i}X^k\otimes X^{n+m-k} = \\
    \sum_{k=0}^{n+m}\binom{n+m}{k}X^k\otimes X^{n+m-k}
\end{gather*}
so $\Delta m = (m \otimes m)(I \otimes T \otimes I)(\Delta \otimes \Delta)$.
\item for all $n, m \in \mathbb{N}$ that $n + m \geq 1$ (w.l.o.g. $m \geq 1$)
\begin{gather*}
    \varepsilon m(X^n \otimes X^m) = \varepsilon (X^{n+m}) = 0 \mathrm{\ and}\\
    m_K(\varepsilon \otimes \varepsilon)(X^n \otimes X^m) = m_K(\varepsilon(X^n)\otimes 0) = 0
\end{gather*}
and for $n = 0$, $m = 0$
\begin{gather*}
    \varepsilon m(X^0 \otimes X^0) = \varepsilon (1_P \otimes 1_P) = 1_K \mathrm{\ and}\\
    m_K(\varepsilon \otimes \varepsilon)(X^0 \otimes X^0) =
    m_K(\varepsilon \otimes \varepsilon)(1_P \otimes 1_P) = m_K(1_K \otimes 1_K) = 1_K,
\end{gather*}
so $\varepsilon m = m_K(\varepsilon \otimes \varepsilon)$.
\item for all $k \in K$
\begin{gather*}
    \Delta u(k) = \Delta(k1_P) =
    \Delta(kX^0) = k\Delta(X_0) = k(X^0 \otimes X^0) \mathrm{\ and} \\
    (u\otimes u)\Delta_K(k) = (u\otimes u) (1_K \otimes k) = k(u \otimes u)(1_K \otimes 1_K) =
    k(1_P \otimes 1_P) = k(X^0 \otimes X^0),
\end{gather*}
so $\Delta u = (u \otimes u)^K\Delta$.
\item for all $k \in K$
\begin{gather*}
    \varepsilon u(k) = k\varepsilon (k1_P) = \varepsilon (kX^0) = k\varepsilon (X^0) = k1_K = k.
\end{gather*}
so $\varepsilon u = I$
\end{enumerate}
It is commutative, because for all $n, m \in \mathbb{N}$
\begin{gather*}
    m(X^n \otimes X^m) = X^{n+m} = X^{m+n} = m(X^m \otimes X^n).
\end{gather*}
and cocommutative, because for all $n \in \mathbb{N}$
\begin{gather*}
    \Delta(X^n) = \sum_{i=0}^n \binom{n}{i} X^i \otimes X^{n - i} = \sum_{i=0} \binom{n}{n-i}X^{n-i} \otimes X^i
\end{gather*}
It is graded as $P = \bigoplus_{i=0}^\infty \mathrm{Lin}(X_0, \dots, X_i)$ \\
and connected as for all $n \in \mathbb{N}$
\begin{gather*}
    m(X^n \otimes X^0) = X^{n+0} = X^n,
\end{gather*}
so $X^0 =1_P$ ($X^0$ is an identity element of $m$).
\section{Graded, connected, associative Hopf algebras of variables}\label{gcha}
These are the main examples of our interest. They will be used to describe inverse and forward riffle
shuffling.\\[4pt]
\textbf{Notational remark.} Let  $\mathcal{X} = \{x_1, \dots, x_N\}$ be a finite set.
Let $\mathcal{X}^*$ - all finite words over an alphabet $\mathcal{X}$.
Let $s, s_1,\dots, s_n \in \mathcal{X}^*$ We will write $s_1 \prec s$ for "$s_1$ is a subword of $s$"
(here a subword does not have to be a contiguous fragment). For $s_1, \dots, s_n \prec s$
we say that $s_1, \dots, s_n$ are pairwise disjoint in $s$ if there exits their occurrences in $s$ such
that no pair of them share any position in $s$. We say that $s$ is a disjoint union of $s_1, \dots, s_n$
if there exists occurrences $s_1, \dots, s_n$ in $s$ such that every position in $s$ is occupied by
exactly one of the occurrences of $s_1, \dots, s_n$. We denote that as $s_1*\dots *s_n=s$.
\subsection{Free associative Hopf algebra}
Let $K$ be a field with characteristic 0.
Let $\mathcal{X} = \{x_1, \dots, x_N\}$ be a finite set. For every $n \in \mathbb{N}$ let
$\mathcal{H}_n$ be a vector space having as a basis all words of length $n$ made of elements
of $\mathcal{X}$. (The basis of $\mathcal{H}_0$ is a singleton of an empty word).
Let $\mathcal{H} \coloneqq \displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i$. Hence
the basis of $\mathcal{H}$ is $\mathcal{X}^*$ - all finite words over an alphabet $\mathcal{X}$.
Let $m : \mathcal{H} \otimes \mathcal{H} \to \mathcal{H}$ be the concatenation of words,
that is, for all $s_1, s_2 \in \mathcal{X}^*$
\begin{equation*}
m(s_1 \otimes s_2) \coloneqq s_1s_2.
\end{equation*}
Let $\Delta : \mathcal{H} \to \mathcal{H} \otimes \mathcal{H}$ be defined for all elements from
$\mathcal{X}$ as
\begin{equation*}
\Delta(x_i) = x_i \otimes 1_\mathcal{H} + 1_\mathcal{H} \otimes x_i.
\end{equation*}
and extends linearly and multiplically .\\
The unit is then $u : K \to \mathcal{H}$ such that
\begin{equation*}
u(1_K) = e
\end{equation*}
where $e$ is an empty word. And indeed $1_\mathcal{H} = e$. \\
And counit is then $\varepsilon : \mathcal{H} \to K$ such that for all $s \in \mathcal{X}^*$: \\
\begin{equation*}
\varepsilon(s) = \begin{cases}
0 \mathrm{\ for\ } s \notin \mathcal{H}_0 \\
1_K \mathrm{\ for\ } s \in \mathcal{H}_0
\end{cases}
\end{equation*}
\textbf{Lemma. } Then $\mathcal{H}$ is a graded, connected Hopf algebra that is cocommutative.
\begin{proof}
Associativity of $m$ and coassociativity of $\Delta$ are obvious. Actions fit together,
because we define them so. Algebra is graded straight from definition and connected because an empty word
is an identity element with respect to concatenation multiplication. Cocomutativity can be checked immediately.
\end{proof}
\noindent Let $s = x_{i_0}\dots x_{i_k} \in \mathcal{X}^*$. What is not so obvious is
how $\Delta(x_{i_0}\dots x_{i_k})$ looks like:
\begin{align}
\Delta(x_{i_0}\dots x_{i_k}) &= \Delta m^{[k]}(x_{i_0} \otimes \dots \otimes x_{i_k}) \\
&= (m^{[k]} \otimes m^{[k]}) \left(\sum (x_{i_0})_1 \otimes \dots \otimes (x_{i_k})_1 \otimes
(x_{i_0})_2 \otimes \dots \otimes (x_{i_k})_2\right) \\
&= \label{siedem}\sum (x_{i_0})_1 \dots (x_{i_k})_1 \otimes
(x_{i_0})_2 \dots (x_{i_k})_2.
\end{align}
It may be unclear what this sum really is. It is taken over all possible combinations of all "possible
values" of $(x_{i_j})_1$ and $(x_{i_j})_2$ for $ 0 \leq j \leq k$. We can recall that for all
$x_i \in \mathcal{X}$ we
have $\Delta(x_i) = x_i \otimes 1_\mathcal{H} + 1_\mathcal{H} \otimes x_i$. Writing that in Sweedler
notation gives
\begin{equation*}
\sum(x_i)_1 \otimes (x_i)_2 = x_i \otimes 1_\mathcal{H} + 1_\mathcal{H} \otimes x_i.
\end{equation*}
The sum we are discussing is then the sum over all possible partitions into two distinct subsequences of $s$,
because for each component of that sum, for each $x_{i_j}$ we decide if we are taking it into the left
subsequence ($x_{i_j}$ as a "value" of $(x_{i_j})_1$ and $1_\mathcal{H}$ as a "value" of $(x_{i_j})_2$) or
into the right subsequence ($1_\mathcal{H}$ as a "value" of $(x_{i_j})_1$ and $x_{i_j}$ as a "value" of
$(x_{i_j})_2$). \\
We can write the sum from \hyperref[siedem]{(7)} as:
\begin{equation*}
\sum (x_{i_0})_1 \dots (x_{i_k})_1 \otimes (x_{i_0})_2 \dots (x_{i_k})_2 =
\sum_{s_1*s_2=s} s_1 \otimes s_2.
\end{equation*}
Equivalently (and that expression can be found in ~\cite{Diaconis2014}) it can be written as
\begin{equation*}
\sum_{S \subseteq \{ i_0, \dots i_k \} } \prod_{j \in S} x_j \otimes \prod_{j \notin S} x_j.
\end{equation*}
where $S$ is a multiset, because some of the $i_0, \dots, i_k$ can be the same. \\
\indent This structure will describe the inverse riffle shuffling, as $\Delta$ will be an operation of
randomly dividing a stack of cards into two stacks by putting each card with probability $\frac{1}{2}$ to
the left or to the right and $m$ will be an operation of deterministic putting the left stack on the top
of the right stack. $\Psi^{[2]}$ will be then an application of one iteration of inverse riffle shuffle and
$\Psi^{[a]}$ will be an application of one iteration of inverse $a$-shuffle.\\
For every $s \in \mathcal{X}^*$ it looks like this:
\begin{equation*}
    \Psi^{[2]}(s) = m\Delta(s) =
    \sum_{s_1*s_2=s}s_1s_2
\end{equation*}
In the similar way for every $s \in \mathcal{X}^*$:
\begin{equation*}
    \Psi^{[a]}(s) = m^{[a]}\Delta^{[a]}(s) =
    \sum_{s_1*\dots *s_a=s}s_1\dots s_a
\end{equation*}

%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////

\subsection{Some further remarks about structure}
In paragraph 2.3 ~\cite{Diaconis2014} describes some aspects of the structure of free associative algebra.
They will be important in the chapter about eigenbases. Here we will present a shortened version for
 lookup.
 \\
~\cite{Garsia_auvancesin} shows that symmetrized sums of certain primitive elements form a basis of a free associative algebra.
It will turn out that this will be the left eigenbasis of $m\Delta$.
Here will be introduced concepts useful for
construction of that basis. Explanation why this is an eigenbasis comes in Chapter 6.
\begin{definition}
A word in an ordered alphabet is \textbf{Lyndon}, if it is strictly smaller (in lexicographical order)
 than all its cyclic rearrangements.
\end{definition}
\begin{definition}
A \textbf{Lyndon factorization} of word $w$ is a tuple of words $(l_1, l_2, \dots, l_k)$ such that
$w = l_1l_2\dots l_k$, each $l_i$ is a Lyndon word and $l_1 \geq l_2 \geq \dots \geq l_k$.
\end{definition}
\noindent\textbf{Fact. ~\cite{Lothaire}(Th. 5.1.5)} Every word $w$ has a unique Lyndon factorisation.
\begin{definition}
For a Lyndon word $l$ that has at least two letters a \textbf{standard factorisation} of $l$ is a pair of
words $(l_1, l_2)$ such that $l = l_1l_2$, both $l_i$ are non-trivial (non-empty) Lyndon words and $l_2$
is the longest right Lyndon factor of $l$. A \textbf{standard factorisation} of a single letter word is that
letter.
\end{definition}
\noindent\textbf{Fact.} Each Lyndon word $l$ has a standard factorization.
\begin{definition}
For a Lyndon word $l$ a \textbf{standard bracketing} $\boldsymbol{\lambda(l)}$ of $l$ is defined recursively
as
$\lambda(a) \coloneqq a$ for a letter and $\lambda(l) \coloneqq [\lambda(l_1), \lambda(l_2)]$, where $(l_1,
l_2)$
is a standard factorisation of $l$. $[x, y] = xy - yx$ for every words $x, y$.
\end{definition}
\begin{definition}
The \textbf{symmetrized product} of word $w$ is
\begin{equation*}
\mathrm{sym}(w) = \sum_{\sigma \in S_k} \lambda(l_{\sigma(1)})\cdot\ldots\cdot\lambda(l_{\sigma(k)}),
\end{equation*}
where $(l_1, \dots, l_k)$ is a Lyndon factorization of $w$.
\end{definition}
~\cite{Garsia_auvancesin}(Th. 5.2) \label{basis}shows that $\{\mathrm{sym}(w) : w \in \mathcal{X^*}\}$ form a basis for free associative
algebra. \\
Let $|w|$ be the length of word $w$. For a word $w = a_1\dots a_{|w|}$ and permutation
$\sigma \in S_{|w|}$ let $\sigma(w) \coloneqq a_{\sigma(1)}\dots a_{\sigma(|w|)}$. \\
Let $\simeq_\mathrm{sym}$ be
a relation on $\mathcal{X}^* \times \mathcal{X}^*$ such that for all $w, v \in \mathcal{X}^*$
\begin{equation*}
w \simeq_\mathrm{sym} v \iff \exists_{\sigma \in S_{|w|}}\ \sigma(w) = v
\end{equation*}
\textbf{Observation.} $\simeq_\mathrm{sym}$ is an equivalence relation on $\mathcal{X}^*$.
\begin{proof}
Obvious.
\end{proof}
\noindent Now we can provide a much finer grading. \\
With every $\nu \in \mathcal{X}^*$ we associate
\begin{equation}
\mathcal{H}_\nu \coloneqq \mathrm{Lin}(\{w \in \mathcal{X}^* : w\ \SimeqSym\ \nu\}).
\end{equation}
So it is the subspace spanned by words that for each letter from $\mathcal{X}$ have the same number of
instances of that letter as
$\nu$. (Of course for every $w, v \in \mathcal{X}^*$ such that $w \SimeqSym v$ we have
$\mathcal{H}_w = \mathcal{H}_v$.) \\
Now we can write $\mathcal{H}$ as
\begin{equation*}
\mathcal{H} = \bigoplus_{{[\nu]}_\SimeqSym \in \mathcal{X}^*_{/\SimeqSym}} \mathcal{H}_\nu
\end{equation*}
Which is equivalent to
\begin{equation*}
\mathcal{H} = \bigoplus_{S \in \mathcal{X}^*_{/\SimeqSym}} \mathrm{Lin}(S)
\end{equation*}
This grading is also compatible with a bialgebra structure we have introduced in the sense that for all
$\nu, \upsilon \in \mathcal{X}^*$
\begin{align*}
m[\mathcal{H}_\nu \otimes \mathcal{H}_\upsilon] &\subseteq \mathcal{H}_{\nu\upsilon} \mathrm{\ and} \\
\Delta[\mathcal{H}_\nu] &\subseteq \bigoplus_{s_1*s_2=\nu}
\mathcal{H}_{s_1} \otimes \mathcal{H}_{s_2}.
\end{align*}
This will be the grading we will be using for our probabilistic interpretation.
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\subsection{Alternative structure    }
\indent Now we will describe an alternative graded and connected Hopf algebra structure on $\mathcal{H}$ -
the
vector space spanned
by finite words over the fixed alphabet $\mathcal{X}$. It will describe the structure of forward riffle
shuffle.
We will denote that alternative Hopf algebra structure
built on $\mathcal{H}$ as $\mathcal{H}^*$ and call it a graded dual of $\mathcal{H}^*$ (for reasons that
will come later).
(Note that $\mathcal{H}$ is isomorphic to $\mathcal{H}^*$ as a vector space and $\mathcal{H}^*$ in this
sense is not the vector space dual to $\mathcal{H}$). \\
We define multiplication $\Delta^* : \mathcal{H}^* \otimes \mathcal{H}^* \to \mathcal{H}^*$
for all $s_1, s_2 \in \mathcal{X}^*$ as:
\begin{equation*}
\Delta^*(s_1 \otimes s_2) = \sum\{s : s_1*s_2 = s\}.
\end{equation*}
which is the sum of all possible interlaces of $s_1$ and $s_2$ \\
and comultiplication $m^* : \mathcal{H}^* \to \mathcal{H}^* \otimes \mathcal{H}^*$ for all $s \in
\mathcal{X}^*$ as:
\begin{equation*}
m^*(s) = \sum\{ s_1 \otimes s_2 : s=s_1s_2 \}
\end{equation*}
which is the sum of all possible divisions of $s$ into its prefix and suffix \\
and both extended linearly. \\
We define Unit $u : K \to \mathcal{H}^*$:
\begin{equation*}
u(1_K) = e
\end{equation*}
where $e$ is an empty word. $1_{\mathcal{H}^*} = e$. \\
And counit $\varepsilon : \mathcal{H}^* \to K$ such that for all $s \in \mathcal{X}^*$: \\
\begin{equation*}
\varepsilon(s) = \begin{cases}
0 \mathrm{\ for\ } s \notin \mathcal{H}_0 \\
1_K \mathrm{\ for\ } s \in \mathcal{H}_0
\end{cases}
\end{equation*}
\textbf{Lemma. } Then $\mathcal{H}^*$ is a graded, connected Hopf algebra that is commutative.
\begin{proof}
Associativity of $\Delta^*$ and coassociativity of $m^*$ are obvious.
Now we will prove that actions fits together which means that
\begin{equation*}
m^*\Delta^* = (\Delta^*\otimes \Delta^*)(I \otimes T \otimes I)(m^* \otimes m^*)
\end{equation*}
Let $g, h \in \mathcal{X}^*$, $m^*\Delta^*(g \otimes h)$ and $(\Delta^*\otimes \Delta^*)(I \otimes T \otimes
I)(m^* \otimes m^*)(g \otimes h)$ are sums of terms of shape $x \otimes y$. We make every letter in
$g$ and $h$ different by giving them specific labels. We will show that then every term has
a coefficient one in that sums, and then, that these terms are the same. Putting labels down will result in
summation of some terms but as they are the same with the labels, they will be the same without them.\\
%Now we will think about words from $\mathcal{X}^*$ as a decks of cards.
\indent Each term in $m^*\Delta^*$ case corresponds to a pair of: possible interlace of $g$ and $h$, and
then, a possible division of its outcome. We want to show that for every term occurring in that sum, there
is only one pair of interlace and division that leads to that term. Hence all terms will have a coefficient
$1_K$. Indeed - if the interlaces are different it means that at least two letters are in different order.
After division they either will be in the different words (which points out the difference) or in one word in
different order (which points out the difference too). If interlaces are the same divisions also must be
the same
to create a specific pair of words. \\
\indent Each term in $(\Delta^*\otimes \Delta^*)(I \otimes T \otimes I)(m^* \otimes m^*)$ case corresponds to
a pair of pairs: two divisions - one of $g$ and one of $h$, and then, two interlaces - one interlace of
created prefixes of $g$ and $h$ and one interlace of created suffixes of $g$ and $h$.
There is only one pair of pairs of divisions and interlaces that leads to a specific term. If at least one
division is different it will lead to a words containing letters with different labels. If divisions are
the same and at least one interlace is different it will lead to word with different order. \\
\indent Now we will show that terms in $m^*\Delta^*(g \otimes h)$ and $(\Delta^*\otimes \Delta^*)(I \otimes T
\otimes I)(m^* \otimes m^*)(g \otimes h)$ are the same. \\
We will do it by showing that for every pair of interlace and division (from $m^*\Delta^*$) there exist
one pair of pairs of interlaces and divisions (from $(\Delta^*\otimes \Delta^*)(I \otimes T \otimes I)(m^*
\otimes m^*)$) that leads to the same term and that for every pair of pairs of interlaces and divisions (from
$(\Delta^*\otimes \Delta^*)(I \otimes T \otimes I)(m^*\otimes m^*)$) there exist one pair of interlace and
 division (from
$m^*\Delta^*$) that leads to the same term. \\
The pair of interlace and division from $m^*\Delta^*$ generates divisions of $g$ and $h$ as "that letters
that went to the prefix" and "that letters that went to the suffix" and interlaces of that prefixes and
 suffixes of $g$ and $h$ that are primal interlace restricted to a part of word. \\
Having pair of pairs of divisions and interlaces from $(\Delta^*\otimes \Delta^*)(I \otimes T \otimes I)
(m^*\otimes m^*)$ we can construct a corresponding interlace of $g \otimes h$ by making that two interlaces
at once. Then the division can be done such that restricted to word $g$ and word $h$ is the same as one of
the original pair. \\
Other properties of bialgebra are easy to check.
Algebra is connected because an empty word
is still an identity element with respect to multiplication. Commutativity is clear.
\end{proof}
In the shuffling interpretation $\Delta^*$ will be the operation of dividing a stack of cards at some random
point and putting the top pile on the left creating two stacks. $m^*$ will be the operation of combining two
stacks together with the same probability of every possible interlace of two stacks.
 ${\Psi^*}^{[2]}$ will be then application of one iteration of forward riffle shuffle and
 ${\Psi^*}^{[a]}$ will be an application of one iteration of forward $a$-shuffle. \\
For every $s \in \mathcal{X}^*$, it looks like this:
\begin{gather*}
{\Psi^*}^{[2]}(s) = \Delta^* m^*(s) = \Delta^* \sum\{s_1 \otimes s_2 : s=s_1s_2\} = \\
\sum\{\Delta^*(s_1 \otimes s_2) : s=s_1s_2\} = \\
\sum\{\sum\{s': s_1*s_2=s'\}:s=s_1s_2\} = \\
\sum\{s' : s=s_1s_2\mathrm{\ and\ }s_1*s_2=s'\}.
\end{gather*}
In the similar way, for every $s \in \mathcal{X}^*$:
\begin{gather*}
{\Psi^*}^{[a]}(s)={\Delta^*}^{[a]} {m^*}^{[a]}(s) = {\Delta^*}^{[a]}\sum\{s_1 \otimes \dots \otimes s_a :
s=s_1\dots s_a\} = \\
\sum\{{\Delta^*}^{[a]}(s_1 \otimes\dots \otimes s_a) : s=s_1\dots s_a\} = \\
\sum\{\sum\{s' : s_1*\dots*s_a = s'\} : s=s_1\dots s_a\} = \\
\sum\{s' : s=s_1\dots s_a\mathrm{\ and\ } s_1* \dots *s_a = s' \}
\end{gather*}
\subsubsection{Graded dual}
Now we will see that there is another method of introducing that structure.
The structure of $\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$ (where for all natural $i$
$\mathcal{H}_i^*$ is a vector space dual to $\mathcal{H}$) with actions induced by actions
from Hopf algebra $\mathcal{H}$ turns out to be one described above. (Note that $\mathcal{H} =
\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i$ is isomorphic as a linear space
to $\displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$.) \\
\indent Let $\gdd{\mathcal{H}} \coloneqq \displaystyle\bigoplus^{\infty}_{i = 0} \mathcal{H}_i^*$.
We define multiplication
\text{$\Delta^* : \mathcal{H}^{\mathrm{gd}*} \otimes \gdd{\mathcal{H}} \to \gdd{\mathcal{H}}$} and
comultiplication
\text{$m^* : \gdd{\mathcal{H}} \to \gdd{\mathcal{H}} \otimes \gdd{\mathcal{H}}$} as
(for all $a^*, b^*, \in \gdd{\mathcal{H}}$):
\begin{align*}
\Delta^*(a^* \otimes b^*) &= (a^* \otimes b^*)\Delta, \\
m^*(a^*) &= a^*m.
\end{align*}
Unit $u_{\gdd{\mathcal{H}}} : K \to \mathcal{H}^*$ and counit $\varepsilon_{\gdd{\mathcal{H}}}
: \mathcal{H}^* \to K$:
\begin{equation*}
u_{\gdd{\mathcal{H}}}(1_K) = e^*
\end{equation*}
where $e$ is an empty word \\
for all $s \in \mathcal{X}^*$
\begin{equation*}
\varepsilon_{\gdd{\mathcal{H}}}(s^*) = \begin{cases}
0 \mathrm{\ for\ } s^* \notin \mathcal{H}^*_0 \\
1_K \mathrm{\ for\ } s^* \in \mathcal{H}^*_0
\end{cases}
\end{equation*}
\indent\textbf{Reminder. } For a vector space $V$ with basis $\mathcal{B}$,
$b \in \mathcal{B}$ let $b^*$
denote a linear functional such that $b^*(b) = 1$ and
$\forall_{b' \in \mathcal{B}\setminus \{b\}}\ b^*(b') = 0$. \\[8pt]
Now we will show that such defined multiplication and comultiplication are the same as defined previously
in the sens that $\gdd{\mathcal{H}} \simeq \mathcal{H}^*$ as a Hopf algebras with an isomorphism $\varphi$
that for all $a \in \mathcal{X}^*$ maps $a \mapsto a^*$.
\begin{theorem}
    For $\mathcal{H}^*$ and $\gdd{\mathcal{H}}$ as defined in this section $\varphi : \mathcal{H}^* \to
    \gdd{\mathcal{H}}$ is an isomorphism of Hopf algebras.
\end{theorem}
\begin{proof}
For all $a, b \in \mathcal{X}^*$:
\begin{gather*}
\Delta^*(a^* \otimes b^*)(s) = (a^* \otimes b^*)\Delta(s) =
(a^* \otimes b^*)\left(\sum_{\substack{s_1*s_2=s}} s_1 \otimes s_2 \right)=\\
\sum_{\substack{s_1*s_2=s}} a^*(s_1) \otimes b^*(s_2)
\end{gather*}
which is equal to one on those $s$ for which holds $a*b=s$, and zero on all other elements
of $s$, so indeed
\[
\Delta^*(a^* \otimes b^*) =
\sum\{s^* : a*b = s\}.
\]
So we have:
\[
\Delta^*(\varphi[a]\otimes \varphi[b]) = \Delta^*(a^* \otimes b^*) =
\sum\{s^* : a*b = s\} = \varphi\left[\Delta^*(a \otimes b)\right].
\]
As well:
\begin{equation*}
    m^*(a^*)(s_1 \otimes s_2) = a^*m(s_1 \otimes s_2) =
    a^*(s_1s_2)
\end{equation*}
which is one only for $s_1$, $s_2$ such that $s_1s_2 = a$, so indeed
\[
m^*(a^*) = \sum\{ s_1^* \otimes s_2^* : a=s_1s_2 \}.
\]
and we have:
\[
m^*(\varphi[a]) = m^*(a^*) = \sum\{ s_1^* \otimes s_2^* : a=s_1s_2 \} = \varphi[m^*(a)].
\]
And for unit and counit, for every $s \in \mathcal{X}^* \setminus \{e\}$
indeed we have ($e$ is an empty word):
\begin{gather*}
\varphi[u_{\mathcal{H}^*}(1_K)] = \varphi[e] =  e^* =u_{\mathcal{\gdd{\mathcal{H}}}}(1_K), \\
\varepsilon_{\gdd{\mathcal{H}}}(\varphi[e]) =
\varepsilon_{\gdd{\mathcal{H}}}(e^*) = 1_K =
\varepsilon_{\mathcal{H}^*}(e), \\
\varepsilon_{\gdd{\mathcal{H}}}(\varphi[s]) =
\varepsilon_{\gdd{\mathcal{H}}}(s^*) = 0 =
\varepsilon_{\mathcal{H}^*}(s)
\end{gather*}
\end{proof}
That gives us, that for all $\nu \in \mathcal{X}^*$, functions $\Psi$
and $\Psi^*$ (defined like in 4.3) treated as linear transformations of a vector space $\mathcal{H}_\nu$
are conjugate
(and that is the reason why we are denoting them with "$^*$"). \\
What is more we have that for all $a \in \mathbb{N}$:
\begin{gather*}
    {\Delta^*}^{[a]}(a^* \otimes b^*) = (a^* \otimes b^*){\Delta^*}^{[a]}, \\
    {m^*}^{[a]}(a^*)=a^*m^{[a]}
\end{gather*}
So as well $\Psi^{[a]}$ and ${\Psi^*}^{[a]}$ are conjugate to each other as linear transformations of
$\mathcal{H}_\nu$.\\ That gives us that their matrices written in basis of $\mathcal{H}_\nu$ are transpositions
to each other.
%//////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\section{Connection}\label{three}
\indent In the chapter one we said that with every Markov chain  have corresponding matrix of transition
probabilities. Thus with a Markov chain one can associate a certain linear transformation given by that
matrix. The following theorem states dependency in opposite direction - that given linear transformation
with some features gives a Markov chain with the basis of transformed vector space as a state space. \\[8pt]
\textbf{Definition.}
For a linear space $V$ over field $K$, with basis $\mathcal{B}$ and its dual basis $\mathcal{B}^*$
we define a linear function $\sigma_\mathcal{B} : V \to K$ as:
\begin{equation*}
    \sigma_\mathcal{B}(v)=\sum_{b_i^*\in\mathcal{B}^*}b_i^*(v)
\end{equation*}
$\sigma_\mathcal{B}(v)$ is then a sum of coefficients standing by elements of $\mathcal{B}$ in $v$
written in $\mathcal{B}$.
\begin{theorem}\label{theorem}
Let $V$ be a linear space over field $K$ that is $\mathbb{R}$ or $\mathbb{Q}$,
 with basis $\mathcal{B}$.
Let $\psi : V \to V$ be a linear operation such that for
all $b \in \mathcal{B}$ coefficients of vector $\psi(b)$ written in $\mathcal{B}$ are greater than $0$
and their
sum $s$ is the same
$(\forall_{b_1, b_2 \in \mathcal{B}}\ b_1^*\psi(b_2) \geq 0 \land \exists_{s \in K}\ \forall_{b \in
\mathcal{B}}\ \sigma_\mathcal{B}\psi(b) = s)$. \\
Let $x_0 \in V\setminus\{0 \}$ and
 $s_0 \coloneqq
\sigma_\mathcal{B}(x_0)$. \\
Let $(\mathcal{B}^\mathbb{N}, \mathbb{P})$ be a probabilistic space such that
for every $n \in \mathbb{N}$, every $\ndotsm{b}{0}{n} \in \mathcal{B}$:
\begin{equation*}
\mathbb{P}(X_0 = b_0, X_1 = b_1, \dots, X_n = b_n) =
\frac{b_0^*(x_0)}{s_0}\prod_{i = 0}^{n-1}\frac{b_{i+1}^*\psi(b_i)}{s}.
\end{equation*}
(where for all $i \in \mathbb{N}$
$X_i : \mathcal{B}^\mathbb{N} \to \mathcal{B}$ is a projection on the $i$-th co-ordinate of
$\mathcal{B}^\mathbb{N}$.) \\
Then $(X_0, X_1, \dots)$ is a Markov chain with state space $\mathcal{B}$
in which, for all $b_1, b_2 \in \mathcal{B}$, the probability of going from $b_1$ to $b_2$ is equal to the
coefficient standing by $b_2$ in $\psi(b_1)$, written in $\mathcal{B}$, divided by $s$.
\end{theorem}
\begin{proof}
Definition is valid,
because for all $n \in \mathbb{N}$, all $\ndotsm{b}{0}{n} \in \mathcal{B}$:
\begin{gather*}
\sum_{b \in \mathcal{B}}\mathbb{P}(X_0 = b_0, X_1 = b_1, \dots, X_n = b_n, X_{n+1} = b) =
\sum_{b \in \mathcal{B}}\frac{b_0^*(x_0)}{s_0}\prod_{i = 0}^{n-1}
\frac{b_{i+1}^*\psi(b_i)}{s}\frac{b^*\psi(b_n)}{s} = \\
\frac{b_0^*(x_0)}{s_0}\prod_{i=0}^{n-1}\frac{b_{i+1}^*\psi(b_i)}{s}\sigma_\mathcal{B}\left(\frac{\psi(b_n)}
{s}\right) =
\mathbb{P}(X_0 = b_0, X_1 = b_1, \dots, X_n = b_n)\frac{\sigma_\mathcal{B}\psi(b_n)}{s} = \\
\mathbb{P}(X_0 = b_0, X_1 = b_1, \dots, X_n = b_n)\frac{s}{s}  = \mathbb{P}(X_0 = b_0, X_1 = b_1, \dots,
X_n = b_n).
\end{gather*}
Now we will show that $(X_0, X_1, \dots)$ is indeed a Markov chain. We will check the $(1.1)$ property from
Chapter 1. \\
Let $\ndotsm{b}{0}{m} \in \mathbb{B}$ be a sequence such that
\begin{equation*}
    \mathbb{P}(X_0 = b_0, \dots, X_m = b_m) > 0
\end{equation*}
We have:
\begin{gather*}
    \mathbb{P}(X_m = b_m \mid X_0 = b_0, \dots, X_{m-1} = b_{m-1}) = \\[16pt]
    \frac{\mathbb{P}(X_0 = b_0, X_1 = b_1, \dots, X_{m-1}
    = b_{m-1}, X_m = b_m)}{\mathbb{P}(X_0 = b_0, \dots, X_{m-1} = b_{m-1})} = \\[16pt]
    \frac{\frac{b_0^*\psi(x_0)}{s_0}\displaystyle\prod_{i = 0}^{m-1}\frac{b_{i+1}^*\psi(b_i)}{s}}
    {\frac{b_0^*\psi(x_0)}{s_0}\displaystyle\prod_{i=0}^{m-2}\frac{b_{i+1}^*\psi(b_i)}{s}} =
    \frac{b_m^*\psi(b_{m-1})}{s}.
\end{gather*}
And on the other hand:
\begin{gather*}
\mathbb{P}(X_m = b_m \mid X_{m-1} = b_{m-1}) = \frac{\mathbb{P}(X_{m-1} = b_{m-1}, X_m = b_m)}
{\mathbb{P}(X_{m-1} = b_{m-1})} = \\[16pt]
\frac{\displaystyle\sum_{(c_0, \dots, c_{m-2}) \in \mathcal{B}^{m-1}}
\mathbb{P}(X_0 = c_0, \dots, X_{m-2} = c_{m-2}, X_{m-1} = b_{m-1}, X_m = b_m)}
{\displaystyle\sum_{(c_0, \dots, c_{m-2}) \in \mathcal{B}^{m-1}}\mathbb{P}(X_0 = c_0,
\dots, X_{m-2} = c_{m-2}, X_{m-1} = b_{m-1})} = \\[16pt]
\frac{\displaystyle\sum_{(c_0, \dots, c_{m-2}) \in \mathcal{B}^{m-1}}\frac{b_0^*\psi(x_0)}{s_0}
\left(\displaystyle\prod_{i = 0}^{m-3}\frac{c_{i+1}^*\psi(c_i)}{s}\right)\frac{b_{m-1}^*\psi(c_{m-2})}{s}
\frac{b_m^*\psi(b_{m-1})}{s}}{\displaystyle\sum_{(\ndotsm{c}{0}{m-2}) \in \mathcal{B}^{m-1}}
\frac{b_0^*\psi(x_0)}{s_0}
\left(\prod_{i= 0}^{m-3}\frac{c_{i+1}^*
\psi(c_i)}{s}\right)\frac{b_{m-1}^*\psi(c_{m-2})}{s}} = \\[16pt]
\frac{\frac{b_m^*\psi(b_{m-1})}{s}\displaystyle\sum_{(c_0, \dots, c_{m-2}) \in \mathcal{B}^{m-1}}
\frac{b_0^*\psi(x_0)}{s_0}
\left(\displaystyle\prod_{i = 0}^{m-3}\frac{c_{i+1}^*\psi(c_i)}{s}\right)\frac{b_{m-1}^*\psi(c_{m-2})}{s}}
{\displaystyle\sum_{(\ndotsm{c}{0}{m-2}) \in \mathcal{B}^{m-1}}\frac{b_0^*\psi(x_0)}{s_0}
\left(\prod_{i= 0}^{m-3}\frac{c_{i+1}^*
\psi(c_i)}{s}\right)\frac{b_{m-1}^*\psi(c_{m-2})}{s}} = \frac{b_m^*\psi(b_{m-1})}{s}
\end{gather*}
and the chain indeed has claimed transition probability, because for all $b_1, b_2 \in \mathcal{B}$,
 $\frac{b_2^*\psi(b_1)}{s}$ is a coefficient
standing by $b_2$ in $\psi(b_1)$, written in $\mathcal{B}$, divided by $s$.
\end{proof}

Free assocciative, cocommutative Hopf algebra of non-commuting variables (from 2.7.1) is a model for inverse
riffle shuffling and its graded dual: commutative
Hopf algebra of non-cocommutative variables (from 2.7.3) is a model for forward riffle shuffling.
It goes as follows.
Let $\mathcal{X}$ be the finite set of all possible types of cards. Let $\nu$ be a tuple of
elements from $\mathcal{X}$ that represents our actual deck of cards (the same type of card can occur
multiple times, the order in $\nu$ should be the order in which we think cards are ordered), the height of
the deck is then $|\nu|$. Now we can take a look at a subspace $\mathcal{H}_\nu$ of vector space
$\mathcal{H}$ built over $\mathcal{X}$ as described in 2.6.2. $\mathcal{H}_\nu$ will be the subspaces
spanned by words that for every type of cards consists of exactly the same number of cards of that type as
$\nu$. So the basis of $\mathcal{H}_\nu$ will be the set of words for every arrangement of our deck of cards.
Let's name this basis $\mathcal{B}_\nu$. Note that then $\mathcal{H}_\nu$ is finite dimensional.
As was previously described, there are two ways of equipping $\mathcal{H}$ with Hopf algebra structure.
One will correspond to inverse version of riffle shuffle and another one to the forward one. With given
arrangement of cards $s \in \mathcal{B}_\nu$ applying $\Psi = m\Delta$ to $s$ yields the sum of possible outcomes
after one inverse riffle shuffle while applying $\Psi^*\Delta^*m^*$ yields the same for forward riffle shuffle.
In both cases coefficients (after normalization) are probabilities of corresponding outcomes. \\[4pt]

As stated in section \ref{gcha} for all $\nu \in \mathcal{X}^*$ for all $b \in \mathcal{B}_\nu$,
we have that coefficients standing by elements of $\mathcal{B}_\nu$
in $\Psi(b)$ written in $\mathcal{B}_\nu$ sums up to $2^{|\nu|}$ so is constant in any particular
$\mathcal{H}_\nu$. What is more, for all $b$ the coefficient standing by $b$ in $\Psi(b)$ is equal to
$|\nu|$ and every other non-zero coefficient in $\Psi(b)$ written in $\mathcal{B}$ is equal to $1$.
The same stands for $\Psi^*$. Because of that, for all $\nu \in \mathcal{X}^*$ we have that
$\mathcal{H}_\nu$ with $\Psi$ and $\mathcal{H}_\nu$ with $\Psi^*$ are satisfying assumptions of
theorem \ref{theorem}, so they sets Markov chains on $\mathcal{B}_\nu$. As stated in the theorem,
for all $b \in \mathcal{B}_\nu$ the probability of going from $b$ to $b$ will be in this case equal to
$|\nu|/2^{|\nu|})$ and probability to any other accessible $b' \in \mathcal{B}_\nu$ will be $1/2^\nu$. \\
Analysis of $\mathcal{H}$ and $\mathcal{H}^*$ (see \ref{gcha}) gives that accessible states are exactly
the same as in, inverse and forward riffle shuffle, respectively. So generated Markov chains are exactly
the one for inverse and forward riffle shuffle.
Further analyses gives that for any $a \in \mathbb{N}$ coefficients of $\Psi^{[a]}(b)$ and ${\Psi^*}^{[a]}(b)$
matches with transition probabilities from $b$ to corresponding outcomes. The sum of coefficients also
depends only on $|\nu|$ for a chosen $\nu$. So $\Psi^{[a]}$ and ${\Psi^*}^{[a]}$ set Markov chains as well
and that chains are the chains of inverse $a$-shuffling and forward $a$-shuffling respectively.

\begin{theorem}
For the fixed deck of cards $\nu$ and natural number $a$
let $(F_0, F_1, \dots)$ and $(I_0, I_1, \dots)$ be Markov chains for, respectively,
forward and inverse riffle $a$-shuffle. Then
\begin{equation*}
\mathbb{P}(F_{n+1} = s_2 \mid F_n = s_1) = \mathbb{P}(I_{n+1} = s_1 \mid I_n = s_2).
\end{equation*}
\end{theorem}
\begin{proof}
    As for all $\nu$ functions $\Psi^{[a]}$ and ${\Psi^*}^{[a]}$ treated as linear transformations of
    a vector space $\mathcal{H}_\nu$ are conjugated to each other
    it gives us, that their matrices are transpositions of each other, so matrices of corresponding chains
    are transpositions of each other, so for every $s_1, s_2 \in S$ probability of going from $s_1$ to $s_2$
    in one step in forward riffle shuffle is equal to going from $s_2$ to $s_1$ in one step in inverse riffle
    shuffle.
\end{proof}
Above theorem states, that indeed forward and inverse riffle shuffling is one method of shuffling
once applied forward and once backward. \\[8pt]
Note that assumptions in the theorem are important, for example we can not make in that  way  a Markov chain
of decompositions of the deck, like applying $\Delta$ and then $\Delta \otimes I$. That will give
$\Delta^{[3]}$ where all possible scenarios of decompositions have equal probabilities.
It is different than splitting the deck in two and then randomly one of these halves.
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
%/////////////////////////////////////////////////////////////////////////////////////////////////////////
\section{Left and right eigenbases}

The reasons we bother with finding the eigenbasis are described in 1.?.?. For a given deck $\nu$
we will find left and right eigenbases of $\mathcal{H}_\nu$ for inverse and forward riffle shuffling.
Note that because
$m\Delta$ and $\Delta^*m^*$ are dual to each other left eigenbasis for inverse riffle-shuffle is right
eigenbasis for forward riffle-shuffle and right eigenbasis for inverse is left eigenbasis for forward.

\subsection{Left eigenbasis}
Construction of left eigenbasis begins with a general observation that for every Hopf algebra,
primitive elements are eigenvectors of $\Psi^{[a]}$ for every $a \in \mathbb{N}$.
\begin{observation}
For every primitive element $h \in \mathcal{H}$, for every $a \in \mathbb{N}$ it holds that $h$ is an
eigenvector of $\Psi^{[a]}$ with an eigenvalue $a$.
\end{observation}
\begin{proof}
Let $h \in \mathcal{H}$ be a primitive element.
\begin{gather*}
\Psi^{[a]}(h) = \\ m^{[a]}\Delta^{[a]}(h) = \\
m^{[a]}(\sum h_1 \otimes \dots \otimes h_i) = \\
m^{[a]}(ce{\overbrace{h \otimes 1_\mathcal{H} \otimes \dots \otimes 1_\mathcal{H}}^{i\
\mathrm{factors}} +
\overbrace{1_\mathcal{H} \otimes h \otimes \dots \otimes 1_\mathcal{H}}^{a\ \mathrm{factors}} +
\dots +
\overbrace{1_\mathcal{H} \otimes 1_\mathcal{H} \otimes \dots \otimes h}^{a\ \mathrm{factors}}}_{a\
\mathrm{summands}} ) = \\
\underbrace{m^{[a]}(\overbrace{h \otimes 1_\mathcal{H} \otimes \dots \otimes 1_\mathcal{H}}^{a\
\mathrm{factors}}) +
m^{[a]}(\overbrace{1_\mathcal{H} \otimes h \otimes \dots \otimes 1_\mathcal{H}}^{a\ \mathrm{factors}}) +
\dots +
m^{[a]}(\overbrace{1_\mathcal{H} \otimes 1_\mathcal{H} \otimes \dots \otimes h}^{a\ \mathrm{factors}})}_{a\
\mathrm{summands}} = \\
\underbrace{h + \dots + h}_{a\ \mathrm{summands}} = \\
ah
\end{gather*}

\end{proof}
Next observation is presented in ~\cite{Diaconis2014} and it is called the symmetrization lemma.
\begin{theorem}
(Symmetrization lemma). Let $x_1, \dots, x_n$ be primitive elements of any Hopf algebra $\mathcal{H}$, then
$\displaystyle\sum_{\sigma \in S_k} x_{\sigma(1)}\cdot\ldots\cdot x_{\sigma(k)}$ is an eigenvector of
$\Psi^{[a]}$ with eigenvalue $a^k$.
\end{theorem}

Now for the basis introduced in \hyperref[basis]{\textbf{4.2}} to be eigenbasis we only need to check that for
every Lyndon
word $l$, element $\lambda(l)$ is primitive.

\begin{lemma}
For every $x$, $y$ that are primitive, $[x, y]$ is primitive.
\end{lemma}

\begin{proof}
Let $x$, $y$ be primitive elements of a bialgebra $\mathcal{H}$.
\begin{align*}
\Delta([&x, y]) = \\ \Delta(x\cdot y &- y\cdot x) =\\ \Delta (x\cdot y) &- \Delta(y\cdot x) =\\
\Delta m(x \otimes y) &- \Delta m(y \otimes x) =\\
\sum (x_1 \otimes x_2) \cdot (y_1 \otimes y_2) &- \sum (y_1 \otimes y_2) \cdot (x_1 \otimes x_2) = \\
(x \otimes 1_\mathcal{H}) \cdot (y \otimes 1_\mathcal{H}) + (x \otimes 1_\mathcal{H}) \cdot (1_\mathcal{H}
\otimes y)
&+ (1_\mathcal{H} \otimes x) \cdot (y \otimes 1_\mathcal{H}) + (1_\mathcal{H} \otimes x) \cdot
(1_\mathcal{H} \otimes y) + \\
- \Big((y \otimes 1_\mathcal{H}) \cdot (x \otimes 1_\mathcal{H}) + (y \otimes 1_\mathcal{H}) \cdot
(1_\mathcal{H} \otimes x)
&+ (1_\mathcal{H} \otimes y) \cdot (x \otimes 1_\mathcal{H}) + (1_\mathcal{H} \otimes y) \cdot
(1_\mathcal{H} \otimes x) \Big) = \\
x \cdot y \otimes 1_\mathcal{H} + x \otimes y + y \otimes x + 1_\mathcal{H} \otimes x \cdot y
&- \Big( y \cdot x \otimes 1_\mathcal{H} + y \otimes x + x \otimes y + 1_\mathcal{H} \otimes y \cdot x
\Big) = \\
(x \cdot y - y \cdot x) \otimes 1_\mathcal{H} &+ 1_\mathcal{H} \otimes (x \cdot y - y \cdot x) = \\
[x, y] \otimes 1_\mathcal{H} &+ 1_\mathcal{H} \otimes [x, y].
\end{align*}

\end{proof}

Because of Lemma 1. and the fact that single-letter word is a primitive element we have that $\lambda(l)$ is
primitive
for every Lyndon word $l$. Hence
\begin{equation*}
\mathcal{E} \coloneqq\left\{\mathrm{sym}(w) : w \in \mathcal{X}^*\right\}
\end{equation*}
is a left eigenbasis of $\mathcal{H}$ with respect to $\Psi^{[a]}$ for every $a$ with an eigenvalue for every
$w$ equal to $a^{l_w}$, where $l_w$ number of Lyndon factors in the Lyndon factorisation of $w$.
What is more:

\begin{theorem}
Let $\mathcal{H}$ be a free associative Hopf algebra of words over the alphabet $\mathcal{X}$.
Let $\nu \in \mathcal{X}^*$. Then $\mathcal{E}_\nu \coloneqq \{\mathrm{sym}(w) : w \in \mathcal{B}_\nu\}$ is
an eigenbasis of
$\mathcal{H}_\nu$ with respect to $\Psi^{[a]}$ for every $a$.
\end{theorem}

To prove it, we will need a following lemma: \\
\noindent \textbf{Lemma.} $w \in \mathcal{H}_\nu$ iff $\mathrm{sym}(w) \in \mathcal{H}_\nu$
\begin{proof}
We can observe that for every $x, y \in \mathcal{X}^*$ there holds $xy\ \SimeqSym\ yx$. Because of that
for every Lyndon word $l$ we have that $\lambda(l) \in \mathcal{H}_l$. So we have that for every word $w$
with Lyndon factorisation $(l_1,\dots, l_k)$ there holds
$\lambda(l_1)\cdot\ldots\cdot\lambda(l_k) \in \mathcal{H}_w$. From that, for every
$\sigma \in S_k$, we have that $\lambda(l_{\sigma(1)})\cdot\ldots\cdot\lambda(l_{\sigma(k)}) \in
\mathcal{H}_w$.

\end{proof}
\begin{proof}(of the theorem) \\
$\mathcal{E}_\nu \subseteq\mathcal{E}$,
so $\mathcal{E}_\nu$ is linearly independent. $w \in \mathcal{H}_\nu$ iff
$\mathrm{sym}(w) \in \mathcal{H}_\nu$ so $\mathcal{E} \cap \mathcal{H}_\nu = \mathcal{E}_\nu$, so
$\mathcal{E}_\nu$ spans the $\mathcal{H}_\nu$.
\end{proof}
\subsection{Right eigenbasis}
The right eigenbasis can be obtained as written in~\cite{Diaconis2014}: \\
For each Lyndon word $b$, let $f_b$ be the eigenvector of ${\Psi^*}^{[a]}$ of eigenvalue $a$ such that
$f_b(\mathrm{sym}(b))=1$ ans $f_b(\mathrm{sym}(g_{b'}) = 0$ for all other Lyndon $b'$. For each basis
element $b$ of $\mathcal{H}$ with Lyndon factorization $b=b_1\dots b_k$, let
\begin{equation*}
    f_b \coloneqq \frac{1}{A'(b)}f_{b_1}\cdot\ldots\cdot f_{b_k},
\end{equation*}
where the normalisation constant $A'(b)$ is calculated as follows: for each Lyndon basis element $b'$, let
$a'_{b'}(b)$ be the number of times $b'$ occurs in the Lyndon factorisation of $b$, and set $A'(b) =
\prod_{b'}a'_{b'}(b)!$. Then $f_b$ is an eigenvector of ${\Psi^*}^{[a]}$ of eigenvalue $a^k$, and $\{f_b\}$
is the dual basis to $\{g_b\}$. \\
The proof can be found in~\cite{Diaconis2014}.
\section{Summation}
We had shown how to describe certain Markov chain using Hopf algebras. There are other examples of doing so.
Polynomial algebra gives model for rock-breaking. Although with this approach there is some apparatus
to build, it gives benefits of possibility to derive from knowledge of the well-known mathematical
structures, when they appear in the study of some probabilistic actions involving some combinatorial
objects.
Which is a beautiful example of how various branches of mathematics contribute to answering given question.

\bibliography{my_bibliography}{}
\bibliographystyle{alpha}
\end{document}
